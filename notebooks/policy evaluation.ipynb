{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import ray\n",
    "\n",
    "from rl_trading.simulation.env import StockExchangeEnv0\n",
    "from rl_trading.data.indicators import *\n",
    "\n",
    "root_dir = '../exp_results/baseline_correct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "state_config = {\n",
    "        'market_state': ['vwap'],\n",
    "        'technical_indicators': [\n",
    "            (RPC, {}, '1min'),\n",
    "            (EMA, dict(timeperiod=5, normalize=True), '1min'),\n",
    "            (EMA, dict(timeperiod=13, normalize=True), '1min'),\n",
    "            (RSI, dict(timeperiod=7, normalize=True), '1min'),\n",
    "            (BBANDS, dict(timeperiod=10), '1min'),\n",
    "            (EMA, dict(timeperiod=20, normalize=True), '1h'),\n",
    "            (EMA, dict(timeperiod=50, normalize=True), '1h'),\n",
    "            (RSI, dict(timeperiod=14, normalize=True), '1h'),\n",
    "            (BBANDS, dict(timeperiod=20), '1h'),\n",
    "            (MACD_DIFF, dict(fastperiod=12, slowperiod=26, signalperiod=9, normalize=True), '1h'),\n",
    "            (EMA, dict(timeperiod=50, normalize=True), '1d'),\n",
    "            (EMA, dict(timeperiod=200, normalize=True), '1d'),\n",
    "            (RSI, dict(timeperiod=14, normalize=True), '1d'),\n",
    "            (BBANDS, dict(timeperiod=20), '1d'),\n",
    "            (MACD_DIFF, dict(fastperiod=12, slowperiod=26, signalperiod=9, normalize=True), '1d'),\n",
    "        ]}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from rl_trading.utils import load_model\n",
    "\n",
    "def load_checkpoints():\n",
    "    checkpoints = {}\n",
    "    for alg_name in os.listdir(root_dir):\n",
    "        checkpoints[alg_name] = {}\n",
    "        for exp_name in os.listdir(os.path.join(root_dir, alg_name)):\n",
    "            for result_dir in os.listdir(os.path.join(root_dir, alg_name, exp_name)):\n",
    "                if not 'steps' in result_dir:\n",
    "                    continue\n",
    "\n",
    "                n_steps = int(re.findall('steps=([0-9]+)', result_dir)[0])\n",
    "\n",
    "                for file in os.listdir(os.path.join(root_dir, alg_name, exp_name, result_dir)):\n",
    "                    if 'checkpoint' in file:\n",
    "                       checkpoints[alg_name][n_steps] = os.path.join(root_dir, alg_name, exp_name, result_dir, file)\n",
    "    return checkpoints\n",
    "\n",
    "def evaluate_policies(checkpoints):\n",
    "    for alg_name in checkpoints:\n",
    "        for n_steps, checkpoint in checkpoints[alg_name].items():\n",
    "            model = load_model(checkpoint)\n",
    "\n",
    "            sim_env = StockExchangeEnv0(\n",
    "                sim_config={'max_steps': n_steps},\n",
    "                state_config=state_config,\n",
    "                _n_days=1,\n",
    "                seed=42\n",
    "            )\n",
    "            eval_env = StockExchangeEnv0(\n",
    "                sim_config={'max_steps': n_steps},\n",
    "                state_config=state_config,\n",
    "                _n_days=1,\n",
    "                seed=42\n",
    "            )\n",
    "\n",
    "            done = False\n",
    "            reward_total = 0\n",
    "            baseline_reward = 0\n",
    "            state, _ = sim_env.reset()\n",
    "            while not done:\n",
    "                current_idx = eval_env.current_idx\n",
    "                current_price = eval_env.price_data['1min'][current_idx]\n",
    "                next_price = eval_env.price_data['1min'][current_idx + 1]\n",
    "                if next_price > current_price:\n",
    "                    action = 1\n",
    "                elif next_price < current_price:\n",
    "                    action = 2\n",
    "                else:\n",
    "                    action = 0\n",
    "                _, reward, done, _, _ = eval_env.step(action)\n",
    "                baseline_reward += reward\n",
    "\n",
    "                action = model.compute_single_action(state, explore=False)\n",
    "                state, reward, done, _, _ = sim_env.step(action)\n",
    "                reward_total += reward\n",
    "            print('n_steps:', n_steps)\n",
    "            print('baseline_reward:', baseline_reward)\n",
    "            print('reward_total:', reward_total)\n",
    "            del model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-16 00:17:43,124\tWARNING checkpoints.py:109 -- No `rllib_checkpoint.json` file found in checkpoint directory ../exp_results/baseline_correct/DQN/DQN_10M_seed=42/DQN_StockExchangeEnv-v0_ab0f0_00002_2_max_steps=30_2023-06-15_22-12-40/checkpoint_000010! Trying to extract checkpoint info from other files found in that dir.\n",
      "2023-06-16 00:17:43,133\tWARNING algorithm_config.py:635 -- Cannot create DQNConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "2023-06-16 00:17:45,019\tINFO worker.py:1616 -- Started a local Ray instance. View the dashboard at \u001B[1m\u001B[32m127.0.0.1:8265 \u001B[39m\u001B[22m\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=3257509)\u001B[0m 2023-06-16 00:17:55,458\tWARNING env.py:155 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "2023-06-16 00:18:00,843\tWARNING env.py:155 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "2023-06-16 00:18:00,924\tINFO trainable.py:172 -- Trainable.setup took 17.769 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_steps: 30\n",
      "baseline_reward: 43.69773366187292\n",
      "reward_total: 14.533179177537022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(RolloutWorker pid=3258203)\u001B[0m 2023-06-16 00:18:10,853\tWARNING env.py:155 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_steps: 10\n",
      "baseline_reward: 23.910725800918954\n",
      "reward_total: 23.32282505819785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(RolloutWorker pid=3258726)\u001B[0m 2023-06-16 00:18:20,716\tWARNING env.py:155 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_steps: 5\n",
      "baseline_reward: 3.312600487506643\n",
      "reward_total: 3.312600487506643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(RolloutWorker pid=3259230)\u001B[0m 2023-06-16 00:18:30,879\tWARNING env.py:155 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "2023-06-16 00:18:31,217\tINFO trainable.py:172 -- Trainable.setup took 10.095 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_steps: 60\n",
      "baseline_reward: 111.7008164300205\n",
      "reward_total: 80.91736596007468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(RolloutWorker pid=3259781)\u001B[0m 2023-06-16 00:18:41,295\tWARNING env.py:155 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "2023-06-16 00:18:41,635\tINFO trainable.py:172 -- Trainable.setup took 10.163 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_steps: 120\n",
      "baseline_reward: 206.08981530180426\n",
      "reward_total: 129.2678949645051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(RolloutWorker pid=3260349)\u001B[0m 2023-06-16 00:18:51,758\tWARNING env.py:155 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_steps: 300\n",
      "baseline_reward: 631.5881569909507\n",
      "reward_total: 254.78855158085207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(RolloutWorker pid=3260974)\u001B[0m 2023-06-16 00:19:02,778\tWARNING env.py:155 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "2023-06-16 00:19:03,214\tINFO trainable.py:172 -- Trainable.setup took 10.063 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_steps: 600\n",
      "baseline_reward: 1179.012786375477\n",
      "reward_total: 271.1943010224495\n"
     ]
    }
   ],
   "source": [
    "checkpoints = load_checkpoints()\n",
    "evaluate_policies(checkpoints)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
