{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from simulation import SimulationEnv\n",
    "\n",
    "register_env('trading_env-v0', lambda config: SimulationEnv(**config))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 22:58:24,632\tINFO worker.py:1616 -- Started a local Ray instance. View the dashboard at \u001B[1m\u001B[32m127.0.0.1:8265 \u001B[39m\u001B[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.10.11', ray_version='2.4.0', ray_commit='4479f66d4db967d3c9dd0af2572061276ba926ba', address_info={'node_ip_address': '192.168.0.222', 'raylet_ip_address': '192.168.0.222', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2023-05-02_22-58-22_829936_228860/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2023-05-02_22-58-22_829936_228860/sockets/raylet', 'webui_url': '127.0.0.1:8265', 'session_dir': '/tmp/ray/session_2023-05-02_22-58-22_829936_228860', 'metrics_export_port': 56625, 'gcs_address': '192.168.0.222:53235', 'address': '192.168.0.222:53235', 'dashboard_agent_listen_port': 52365, 'node_id': 'a2dd9a0b9993e4c1ce074bf6e75227920773cc13eb4297784a4e44c6'})",
      "text/html": "<div>\n    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n            <g id=\"layer-1\">\n                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n            </g>\n        </svg>\n        <table>\n            <tr>\n                <td style=\"text-align: left\"><b>Python version:</b></td>\n                <td style=\"text-align: left\"><b>3.10.11</b></td>\n            </tr>\n            <tr>\n                <td style=\"text-align: left\"><b>Ray version:</b></td>\n                <td style=\"text-align: left\"><b> 2.4.0</b></td>\n            </tr>\n            <tr>\n    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8265\" target=\"_blank\">http://127.0.0.1:8265</a></b></td>\n</tr>\n\n        </table>\n    </div>\n</div>\n"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 00:06:10,163\tWARNING util.py:67 -- Install gputil for GPU system monitoring.\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=371580)\u001B[0m /home/fassty/anaconda3/envs/diploma_thesis/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=371580)\u001B[0m   if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 4000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.003806880263031506\n",
      "  StateBufferConnector_ms: 0.0031102550485746457\n",
      "  ViewRequirementAgentConnector_ms: 0.07859175322485752\n",
      "counters:\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-06-19\n",
      "done: false\n",
      "episode_len_mean: 21.80327868852459\n",
      "episode_media: {}\n",
      "episode_reward_max: 67.0\n",
      "episode_reward_mean: 21.80327868852459\n",
      "episode_reward_min: 8.0\n",
      "episodes_this_iter: 183\n",
      "episodes_total: 183\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.6642202406160294\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02892561394374421\n",
      "        policy_loss: -0.04225908713715692\n",
      "        total_loss: 8.885406422358686\n",
      "        vf_explained_var: 0.004496763598534369\n",
      "        vf_loss: 8.92188039338717\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 465.5\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "iterations_since_restore: 1\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 4000\n",
      "num_agent_steps_trained: 4000\n",
      "num_env_steps_sampled: 4000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 4000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 22.807142857142857\n",
      "  ram_util_percent: 22.221428571428568\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08145072286768397\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04191971873974865\n",
      "  mean_inference_ms: 0.7149424740029289\n",
      "  mean_raw_obs_processing_ms: 0.22525276073483463\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.003806880263031506\n",
      "    StateBufferConnector_ms: 0.0031102550485746457\n",
      "    ViewRequirementAgentConnector_ms: 0.07859175322485752\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 21.80327868852459\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 67.0\n",
      "  episode_reward_mean: 21.80327868852459\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 183\n",
      "  hist_stats:\n",
      "    episode_lengths: [24, 11, 17, 17, 17, 18, 26, 12, 24, 28, 33, 12, 29, 24, 11,\n",
      "      12, 41, 13, 15, 8, 12, 23, 25, 13, 25, 20, 9, 9, 14, 11, 13, 14, 67, 32, 16,\n",
      "      11, 28, 12, 17, 30, 29, 24, 16, 11, 18, 20, 24, 13, 13, 10, 19, 28, 24, 25,\n",
      "      25, 19, 10, 11, 17, 16, 18, 41, 14, 12, 63, 16, 24, 15, 27, 18, 23, 15, 29,\n",
      "      31, 17, 20, 27, 31, 15, 56, 15, 21, 18, 22, 16, 16, 12, 12, 12, 21, 12, 36,\n",
      "      20, 22, 19, 21, 19, 11, 27, 17, 23, 13, 14, 17, 10, 24, 9, 42, 41, 30, 34, 35,\n",
      "      10, 23, 17, 21, 26, 17, 26, 19, 14, 41, 21, 33, 17, 26, 18, 12, 16, 50, 47,\n",
      "      16, 15, 13, 9, 14, 16, 31, 24, 15, 31, 37, 13, 47, 18, 12, 14, 39, 9, 30, 32,\n",
      "      15, 18, 12, 15, 22, 27, 49, 14, 27, 15, 29, 34, 35, 22, 13, 12, 24, 12, 62,\n",
      "      17, 30, 19, 17, 35, 12, 42, 12, 21, 18, 32, 19, 32]\n",
      "    episode_reward: [24.0, 11.0, 17.0, 17.0, 17.0, 18.0, 26.0, 12.0, 24.0, 28.0, 33.0,\n",
      "      12.0, 29.0, 24.0, 11.0, 12.0, 41.0, 13.0, 15.0, 8.0, 12.0, 23.0, 25.0, 13.0,\n",
      "      25.0, 20.0, 9.0, 9.0, 14.0, 11.0, 13.0, 14.0, 67.0, 32.0, 16.0, 11.0, 28.0,\n",
      "      12.0, 17.0, 30.0, 29.0, 24.0, 16.0, 11.0, 18.0, 20.0, 24.0, 13.0, 13.0, 10.0,\n",
      "      19.0, 28.0, 24.0, 25.0, 25.0, 19.0, 10.0, 11.0, 17.0, 16.0, 18.0, 41.0, 14.0,\n",
      "      12.0, 63.0, 16.0, 24.0, 15.0, 27.0, 18.0, 23.0, 15.0, 29.0, 31.0, 17.0, 20.0,\n",
      "      27.0, 31.0, 15.0, 56.0, 15.0, 21.0, 18.0, 22.0, 16.0, 16.0, 12.0, 12.0, 12.0,\n",
      "      21.0, 12.0, 36.0, 20.0, 22.0, 19.0, 21.0, 19.0, 11.0, 27.0, 17.0, 23.0, 13.0,\n",
      "      14.0, 17.0, 10.0, 24.0, 9.0, 42.0, 41.0, 30.0, 34.0, 35.0, 10.0, 23.0, 17.0,\n",
      "      21.0, 26.0, 17.0, 26.0, 19.0, 14.0, 41.0, 21.0, 33.0, 17.0, 26.0, 18.0, 12.0,\n",
      "      16.0, 50.0, 47.0, 16.0, 15.0, 13.0, 9.0, 14.0, 16.0, 31.0, 24.0, 15.0, 31.0,\n",
      "      37.0, 13.0, 47.0, 18.0, 12.0, 14.0, 39.0, 9.0, 30.0, 32.0, 15.0, 18.0, 12.0,\n",
      "      15.0, 22.0, 27.0, 49.0, 14.0, 27.0, 15.0, 29.0, 34.0, 35.0, 22.0, 13.0, 12.0,\n",
      "      24.0, 12.0, 62.0, 17.0, 30.0, 19.0, 17.0, 35.0, 12.0, 42.0, 12.0, 21.0, 18.0,\n",
      "      32.0, 19.0, 32.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08145072286768397\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04191971873974865\n",
      "    mean_inference_ms: 0.7149424740029289\n",
      "    mean_raw_obs_processing_ms: 0.22525276073483463\n",
      "time_since_restore: 9.639703035354614\n",
      "time_this_iter_s: 9.639703035354614\n",
      "time_total_s: 9.639703035354614\n",
      "timers:\n",
      "  learn_throughput: 749.406\n",
      "  learn_time_ms: 5337.558\n",
      "  load_throughput: 4248472.018\n",
      "  load_time_ms: 0.942\n",
      "  sample_time_ms: 4293.948\n",
      "  synch_weights_time_ms: 1.405\n",
      "  training_iteration_time_ms: 9634.434\n",
      "timestamp: 1683065179\n",
      "timesteps_total: 4000\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 8000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0036695003509521484\n",
      "  StateBufferConnector_ms: 0.002985715866088867\n",
      "  ViewRequirementAgentConnector_ms: 0.0801534652709961\n",
      "counters:\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_trained: 8000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-06-30\n",
      "done: false\n",
      "episode_len_mean: 41.11\n",
      "episode_media: {}\n",
      "episode_reward_max: 197.0\n",
      "episode_reward_mean: 41.11\n",
      "episode_reward_min: 9.0\n",
      "episodes_this_iter: 96\n",
      "episodes_total: 279\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.6064180510018462\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0180645804582251\n",
      "        policy_loss: -0.03311150244647457\n",
      "        total_loss: 8.716961056186307\n",
      "        vf_explained_var: 0.059636845319501815\n",
      "        vf_loss: 8.744653209050496\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 1395.5\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_trained: 8000\n",
      "iterations_since_restore: 2\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 8000\n",
      "num_agent_steps_trained: 8000\n",
      "num_env_steps_sampled: 8000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 8000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 26.933333333333334\n",
      "  ram_util_percent: 22.300000000000004\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08134549698510478\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.041864188191480994\n",
      "  mean_inference_ms: 0.7167557549782617\n",
      "  mean_raw_obs_processing_ms: 0.21813052723154122\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0036695003509521484\n",
      "    StateBufferConnector_ms: 0.002985715866088867\n",
      "    ViewRequirementAgentConnector_ms: 0.0801534652709961\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 41.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 197.0\n",
      "  episode_reward_mean: 41.11\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 96\n",
      "  hist_stats:\n",
      "    episode_lengths: [18, 32, 19, 32, 21, 33, 46, 34, 34, 45, 46, 14, 50, 15, 22,\n",
      "      11, 50, 37, 197, 85, 54, 84, 26, 33, 21, 14, 26, 105, 37, 120, 13, 89, 72, 84,\n",
      "      36, 34, 57, 52, 90, 66, 28, 29, 22, 23, 31, 13, 30, 29, 26, 27, 61, 27, 29,\n",
      "      37, 34, 30, 36, 47, 53, 47, 33, 15, 30, 22, 46, 65, 21, 22, 29, 37, 17, 90,\n",
      "      22, 37, 24, 67, 34, 27, 47, 39, 28, 78, 28, 47, 36, 28, 18, 16, 87, 11, 96,\n",
      "      9, 21, 45, 42, 22, 24, 37, 55, 46]\n",
      "    episode_reward: [18.0, 32.0, 19.0, 32.0, 21.0, 33.0, 46.0, 34.0, 34.0, 45.0, 46.0,\n",
      "      14.0, 50.0, 15.0, 22.0, 11.0, 50.0, 37.0, 197.0, 85.0, 54.0, 84.0, 26.0, 33.0,\n",
      "      21.0, 14.0, 26.0, 105.0, 37.0, 120.0, 13.0, 89.0, 72.0, 84.0, 36.0, 34.0, 57.0,\n",
      "      52.0, 90.0, 66.0, 28.0, 29.0, 22.0, 23.0, 31.0, 13.0, 30.0, 29.0, 26.0, 27.0,\n",
      "      61.0, 27.0, 29.0, 37.0, 34.0, 30.0, 36.0, 47.0, 53.0, 47.0, 33.0, 15.0, 30.0,\n",
      "      22.0, 46.0, 65.0, 21.0, 22.0, 29.0, 37.0, 17.0, 90.0, 22.0, 37.0, 24.0, 67.0,\n",
      "      34.0, 27.0, 47.0, 39.0, 28.0, 78.0, 28.0, 47.0, 36.0, 28.0, 18.0, 16.0, 87.0,\n",
      "      11.0, 96.0, 9.0, 21.0, 45.0, 42.0, 22.0, 24.0, 37.0, 55.0, 46.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08134549698510478\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.041864188191480994\n",
      "    mean_inference_ms: 0.7167557549782617\n",
      "    mean_raw_obs_processing_ms: 0.21813052723154122\n",
      "time_since_restore: 19.877743005752563\n",
      "time_this_iter_s: 10.23803997039795\n",
      "time_total_s: 19.877743005752563\n",
      "timers:\n",
      "  learn_throughput: 706.476\n",
      "  learn_time_ms: 5661.909\n",
      "  load_throughput: 3547730.176\n",
      "  load_time_ms: 1.127\n",
      "  sample_time_ms: 4269.202\n",
      "  synch_weights_time_ms: 1.628\n",
      "  training_iteration_time_ms: 9934.422\n",
      "timestamp: 1683065190\n",
      "timesteps_total: 8000\n",
      "training_iteration: 2\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 12000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.003697633743286133\n",
      "  StateBufferConnector_ms: 0.00302886962890625\n",
      "  ViewRequirementAgentConnector_ms: 0.0815575122833252\n",
      "counters:\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_trained: 12000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-06-40\n",
      "done: false\n",
      "episode_len_mean: 66.23\n",
      "episode_media: {}\n",
      "episode_reward_max: 359.0\n",
      "episode_reward_mean: 66.23\n",
      "episode_reward_min: 9.0\n",
      "episodes_this_iter: 30\n",
      "episodes_total: 309\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5880730129698272\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008883781760651311\n",
      "        policy_loss: -0.02229156286645961\n",
      "        total_loss: 9.488214899903985\n",
      "        vf_explained_var: 0.07090149380827462\n",
      "        vf_loss: 9.507841320448025\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 2325.5\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_trained: 12000\n",
      "iterations_since_restore: 3\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 12000\n",
      "num_agent_steps_trained: 12000\n",
      "num_env_steps_sampled: 12000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 12000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 21.264285714285712\n",
      "  ram_util_percent: 22.37142857142857\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0817359877655055\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04215620916175297\n",
      "  mean_inference_ms: 0.7210821082091496\n",
      "  mean_raw_obs_processing_ms: 0.21684615214570052\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.003697633743286133\n",
      "    StateBufferConnector_ms: 0.00302886962890625\n",
      "    ViewRequirementAgentConnector_ms: 0.0815575122833252\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 66.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 359.0\n",
      "  episode_reward_mean: 66.23\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 30\n",
      "  hist_stats:\n",
      "    episode_lengths: [13, 89, 72, 84, 36, 34, 57, 52, 90, 66, 28, 29, 22, 23, 31,\n",
      "      13, 30, 29, 26, 27, 61, 27, 29, 37, 34, 30, 36, 47, 53, 47, 33, 15, 30, 22,\n",
      "      46, 65, 21, 22, 29, 37, 17, 90, 22, 37, 24, 67, 34, 27, 47, 39, 28, 78, 28,\n",
      "      47, 36, 28, 18, 16, 87, 11, 96, 9, 21, 45, 42, 22, 24, 37, 55, 46, 114, 123,\n",
      "      120, 211, 95, 143, 123, 75, 166, 259, 151, 149, 220, 34, 178, 75, 107, 158,\n",
      "      359, 135, 18, 52, 262, 48, 38, 94, 149, 54, 117, 46]\n",
      "    episode_reward: [13.0, 89.0, 72.0, 84.0, 36.0, 34.0, 57.0, 52.0, 90.0, 66.0, 28.0,\n",
      "      29.0, 22.0, 23.0, 31.0, 13.0, 30.0, 29.0, 26.0, 27.0, 61.0, 27.0, 29.0, 37.0,\n",
      "      34.0, 30.0, 36.0, 47.0, 53.0, 47.0, 33.0, 15.0, 30.0, 22.0, 46.0, 65.0, 21.0,\n",
      "      22.0, 29.0, 37.0, 17.0, 90.0, 22.0, 37.0, 24.0, 67.0, 34.0, 27.0, 47.0, 39.0,\n",
      "      28.0, 78.0, 28.0, 47.0, 36.0, 28.0, 18.0, 16.0, 87.0, 11.0, 96.0, 9.0, 21.0,\n",
      "      45.0, 42.0, 22.0, 24.0, 37.0, 55.0, 46.0, 114.0, 123.0, 120.0, 211.0, 95.0,\n",
      "      143.0, 123.0, 75.0, 166.0, 259.0, 151.0, 149.0, 220.0, 34.0, 178.0, 75.0, 107.0,\n",
      "      158.0, 359.0, 135.0, 18.0, 52.0, 262.0, 48.0, 38.0, 94.0, 149.0, 54.0, 117.0,\n",
      "      46.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0817359877655055\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04215620916175297\n",
      "    mean_inference_ms: 0.7210821082091496\n",
      "    mean_raw_obs_processing_ms: 0.21684615214570052\n",
      "time_since_restore: 29.96867036819458\n",
      "time_this_iter_s: 10.090927362442017\n",
      "time_total_s: 29.96867036819458\n",
      "timers:\n",
      "  learn_throughput: 706.646\n",
      "  learn_time_ms: 5660.544\n",
      "  load_throughput: 3628029.121\n",
      "  load_time_ms: 1.103\n",
      "  sample_time_ms: 4321.755\n",
      "  synch_weights_time_ms: 1.557\n",
      "  training_iteration_time_ms: 9985.513\n",
      "timestamp: 1683065200\n",
      "timesteps_total: 12000\n",
      "training_iteration: 3\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 16000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.003737211227416992\n",
      "  StateBufferConnector_ms: 0.003033876419067383\n",
      "  ViewRequirementAgentConnector_ms: 0.08196234703063965\n",
      "counters:\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_trained: 16000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-06-50\n",
      "done: false\n",
      "episode_len_mean: 96.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 96.0\n",
      "episode_reward_min: 9.0\n",
      "episodes_this_iter: 15\n",
      "episodes_total: 324\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5659154468967068\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005079066319923096\n",
      "        policy_loss: -0.020619280460060285\n",
      "        total_loss: 9.759084955851238\n",
      "        vf_explained_var: -0.04384885616199945\n",
      "        vf_loss: 9.77818050794704\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 3255.5\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_trained: 16000\n",
      "iterations_since_restore: 4\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 16000\n",
      "num_agent_steps_trained: 16000\n",
      "num_env_steps_sampled: 16000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 16000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 18.842857142857145\n",
      "  ram_util_percent: 22.378571428571426\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08196255582524432\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.042324687361075704\n",
      "  mean_inference_ms: 0.7234130382131886\n",
      "  mean_raw_obs_processing_ms: 0.21588003314468657\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.003737211227416992\n",
      "    StateBufferConnector_ms: 0.003033876419067383\n",
      "    ViewRequirementAgentConnector_ms: 0.08196234703063965\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 96.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 96.0\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 15\n",
      "  hist_stats:\n",
      "    episode_lengths: [13, 30, 29, 26, 27, 61, 27, 29, 37, 34, 30, 36, 47, 53, 47,\n",
      "      33, 15, 30, 22, 46, 65, 21, 22, 29, 37, 17, 90, 22, 37, 24, 67, 34, 27, 47,\n",
      "      39, 28, 78, 28, 47, 36, 28, 18, 16, 87, 11, 96, 9, 21, 45, 42, 22, 24, 37, 55,\n",
      "      46, 114, 123, 120, 211, 95, 143, 123, 75, 166, 259, 151, 149, 220, 34, 178,\n",
      "      75, 107, 158, 359, 135, 18, 52, 262, 48, 38, 94, 149, 54, 117, 46, 237, 228,\n",
      "      262, 325, 62, 110, 194, 344, 121, 115, 448, 391, 500, 259, 107]\n",
      "    episode_reward: [13.0, 30.0, 29.0, 26.0, 27.0, 61.0, 27.0, 29.0, 37.0, 34.0, 30.0,\n",
      "      36.0, 47.0, 53.0, 47.0, 33.0, 15.0, 30.0, 22.0, 46.0, 65.0, 21.0, 22.0, 29.0,\n",
      "      37.0, 17.0, 90.0, 22.0, 37.0, 24.0, 67.0, 34.0, 27.0, 47.0, 39.0, 28.0, 78.0,\n",
      "      28.0, 47.0, 36.0, 28.0, 18.0, 16.0, 87.0, 11.0, 96.0, 9.0, 21.0, 45.0, 42.0,\n",
      "      22.0, 24.0, 37.0, 55.0, 46.0, 114.0, 123.0, 120.0, 211.0, 95.0, 143.0, 123.0,\n",
      "      75.0, 166.0, 259.0, 151.0, 149.0, 220.0, 34.0, 178.0, 75.0, 107.0, 158.0, 359.0,\n",
      "      135.0, 18.0, 52.0, 262.0, 48.0, 38.0, 94.0, 149.0, 54.0, 117.0, 46.0, 237.0,\n",
      "      228.0, 262.0, 325.0, 62.0, 110.0, 194.0, 344.0, 121.0, 115.0, 448.0, 391.0,\n",
      "      500.0, 259.0, 107.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08196255582524432\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.042324687361075704\n",
      "    mean_inference_ms: 0.7234130382131886\n",
      "    mean_raw_obs_processing_ms: 0.21588003314468657\n",
      "time_since_restore: 39.93773007392883\n",
      "time_this_iter_s: 9.969059705734253\n",
      "time_total_s: 39.93773007392883\n",
      "timers:\n",
      "  learn_throughput: 706.527\n",
      "  learn_time_ms: 5661.494\n",
      "  load_throughput: 2676005.423\n",
      "  load_time_ms: 1.495\n",
      "  sample_time_ms: 4315.513\n",
      "  synch_weights_time_ms: 1.575\n",
      "  training_iteration_time_ms: 9980.633\n",
      "timestamp: 1683065210\n",
      "timesteps_total: 16000\n",
      "training_iteration: 4\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 20000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0037953853607177734\n",
      "  StateBufferConnector_ms: 0.0030736923217773438\n",
      "  ViewRequirementAgentConnector_ms: 0.08270001411437988\n",
      "counters:\n",
      "  num_agent_steps_sampled: 20000\n",
      "  num_agent_steps_trained: 20000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_trained: 20000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-07-00\n",
      "done: false\n",
      "episode_len_mean: 132.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 132.58\n",
      "episode_reward_min: 9.0\n",
      "episodes_this_iter: 11\n",
      "episodes_total: 335\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5415201758184741\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0046537948646172105\n",
      "        policy_loss: -0.021797938356476445\n",
      "        total_loss: 9.838188366223408\n",
      "        vf_explained_var: -0.10255163850322846\n",
      "        vf_loss: 9.858590168081305\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 4185.5\n",
      "  num_agent_steps_sampled: 20000\n",
      "  num_agent_steps_trained: 20000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_trained: 20000\n",
      "iterations_since_restore: 5\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 20000\n",
      "num_agent_steps_trained: 20000\n",
      "num_env_steps_sampled: 20000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 20000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 13.7\n",
      "  ram_util_percent: 22.38571428571429\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08216433383924915\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04247205314379692\n",
      "  mean_inference_ms: 0.7254500782849692\n",
      "  mean_raw_obs_processing_ms: 0.2149832515266484\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0037953853607177734\n",
      "    StateBufferConnector_ms: 0.0030736923217773438\n",
      "    ViewRequirementAgentConnector_ms: 0.08270001411437988\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 132.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 132.58\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 11\n",
      "  hist_stats:\n",
      "    episode_lengths: [36, 47, 53, 47, 33, 15, 30, 22, 46, 65, 21, 22, 29, 37, 17,\n",
      "      90, 22, 37, 24, 67, 34, 27, 47, 39, 28, 78, 28, 47, 36, 28, 18, 16, 87, 11,\n",
      "      96, 9, 21, 45, 42, 22, 24, 37, 55, 46, 114, 123, 120, 211, 95, 143, 123, 75,\n",
      "      166, 259, 151, 149, 220, 34, 178, 75, 107, 158, 359, 135, 18, 52, 262, 48, 38,\n",
      "      94, 149, 54, 117, 46, 237, 228, 262, 325, 62, 110, 194, 344, 121, 115, 448,\n",
      "      391, 500, 259, 107, 500, 182, 500, 87, 334, 220, 443, 389, 500, 470, 376]\n",
      "    episode_reward: [36.0, 47.0, 53.0, 47.0, 33.0, 15.0, 30.0, 22.0, 46.0, 65.0, 21.0,\n",
      "      22.0, 29.0, 37.0, 17.0, 90.0, 22.0, 37.0, 24.0, 67.0, 34.0, 27.0, 47.0, 39.0,\n",
      "      28.0, 78.0, 28.0, 47.0, 36.0, 28.0, 18.0, 16.0, 87.0, 11.0, 96.0, 9.0, 21.0,\n",
      "      45.0, 42.0, 22.0, 24.0, 37.0, 55.0, 46.0, 114.0, 123.0, 120.0, 211.0, 95.0,\n",
      "      143.0, 123.0, 75.0, 166.0, 259.0, 151.0, 149.0, 220.0, 34.0, 178.0, 75.0, 107.0,\n",
      "      158.0, 359.0, 135.0, 18.0, 52.0, 262.0, 48.0, 38.0, 94.0, 149.0, 54.0, 117.0,\n",
      "      46.0, 237.0, 228.0, 262.0, 325.0, 62.0, 110.0, 194.0, 344.0, 121.0, 115.0, 448.0,\n",
      "      391.0, 500.0, 259.0, 107.0, 500.0, 182.0, 500.0, 87.0, 334.0, 220.0, 443.0,\n",
      "      389.0, 500.0, 470.0, 376.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08216433383924915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04247205314379692\n",
      "    mean_inference_ms: 0.7254500782849692\n",
      "    mean_raw_obs_processing_ms: 0.2149832515266484\n",
      "time_since_restore: 49.9571168422699\n",
      "time_this_iter_s: 10.019386768341064\n",
      "time_total_s: 49.9571168422699\n",
      "timers:\n",
      "  learn_throughput: 706.514\n",
      "  learn_time_ms: 5661.602\n",
      "  load_throughput: 2838592.312\n",
      "  load_time_ms: 1.409\n",
      "  sample_time_ms: 4322.67\n",
      "  synch_weights_time_ms: 1.534\n",
      "  training_iteration_time_ms: 9987.794\n",
      "timestamp: 1683065220\n",
      "timesteps_total: 20000\n",
      "training_iteration: 5\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 24000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0038285255432128906\n",
      "  StateBufferConnector_ms: 0.003063201904296875\n",
      "  ViewRequirementAgentConnector_ms: 0.08307147026062012\n",
      "counters:\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_trained: 24000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-07-10\n",
      "done: false\n",
      "episode_len_mean: 173.52\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 173.52\n",
      "episode_reward_min: 9.0\n",
      "episodes_this_iter: 9\n",
      "episodes_total: 344\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5670226209266211\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0026482796101175757\n",
      "        policy_loss: -0.021564667293381308\n",
      "        total_loss: 9.905378257587392\n",
      "        vf_explained_var: -0.3049926138052376\n",
      "        vf_loss: 9.926545712255663\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 5115.5\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_trained: 24000\n",
      "iterations_since_restore: 6\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 24000\n",
      "num_agent_steps_trained: 24000\n",
      "num_env_steps_sampled: 24000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 24000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 18.033333333333335\n",
      "  ram_util_percent: 22.3\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08234392379852094\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.042603526120990935\n",
      "  mean_inference_ms: 0.7272532980629546\n",
      "  mean_raw_obs_processing_ms: 0.21413472028032032\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0038285255432128906\n",
      "    StateBufferConnector_ms: 0.003063201904296875\n",
      "    ViewRequirementAgentConnector_ms: 0.08307147026062012\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 173.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 173.52\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 9\n",
      "  hist_stats:\n",
      "    episode_lengths: [65, 21, 22, 29, 37, 17, 90, 22, 37, 24, 67, 34, 27, 47, 39,\n",
      "      28, 78, 28, 47, 36, 28, 18, 16, 87, 11, 96, 9, 21, 45, 42, 22, 24, 37, 55, 46,\n",
      "      114, 123, 120, 211, 95, 143, 123, 75, 166, 259, 151, 149, 220, 34, 178, 75,\n",
      "      107, 158, 359, 135, 18, 52, 262, 48, 38, 94, 149, 54, 117, 46, 237, 228, 262,\n",
      "      325, 62, 110, 194, 344, 121, 115, 448, 391, 500, 259, 107, 500, 182, 500, 87,\n",
      "      334, 220, 443, 389, 500, 470, 376, 500, 423, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [65.0, 21.0, 22.0, 29.0, 37.0, 17.0, 90.0, 22.0, 37.0, 24.0, 67.0,\n",
      "      34.0, 27.0, 47.0, 39.0, 28.0, 78.0, 28.0, 47.0, 36.0, 28.0, 18.0, 16.0, 87.0,\n",
      "      11.0, 96.0, 9.0, 21.0, 45.0, 42.0, 22.0, 24.0, 37.0, 55.0, 46.0, 114.0, 123.0,\n",
      "      120.0, 211.0, 95.0, 143.0, 123.0, 75.0, 166.0, 259.0, 151.0, 149.0, 220.0, 34.0,\n",
      "      178.0, 75.0, 107.0, 158.0, 359.0, 135.0, 18.0, 52.0, 262.0, 48.0, 38.0, 94.0,\n",
      "      149.0, 54.0, 117.0, 46.0, 237.0, 228.0, 262.0, 325.0, 62.0, 110.0, 194.0, 344.0,\n",
      "      121.0, 115.0, 448.0, 391.0, 500.0, 259.0, 107.0, 500.0, 182.0, 500.0, 87.0,\n",
      "      334.0, 220.0, 443.0, 389.0, 500.0, 470.0, 376.0, 500.0, 423.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08234392379852094\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.042603526120990935\n",
      "    mean_inference_ms: 0.7272532980629546\n",
      "    mean_raw_obs_processing_ms: 0.21413472028032032\n",
      "time_since_restore: 59.98573875427246\n",
      "time_this_iter_s: 10.028621912002563\n",
      "time_total_s: 59.98573875427246\n",
      "timers:\n",
      "  learn_throughput: 705.964\n",
      "  learn_time_ms: 5666.009\n",
      "  load_throughput: 2933421.611\n",
      "  load_time_ms: 1.364\n",
      "  sample_time_ms: 4324.682\n",
      "  synch_weights_time_ms: 1.511\n",
      "  training_iteration_time_ms: 9994.141\n",
      "timestamp: 1683065230\n",
      "timesteps_total: 24000\n",
      "training_iteration: 6\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 28000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.003958940505981445\n",
      "  StateBufferConnector_ms: 0.0030775070190429688\n",
      "  ViewRequirementAgentConnector_ms: 0.08319807052612305\n",
      "counters:\n",
      "  num_agent_steps_sampled: 28000\n",
      "  num_agent_steps_trained: 28000\n",
      "  num_env_steps_sampled: 28000\n",
      "  num_env_steps_trained: 28000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-07-20\n",
      "done: false\n",
      "episode_len_mean: 207.58\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 207.58\n",
      "episode_reward_min: 9.0\n",
      "episodes_this_iter: 14\n",
      "episodes_total: 358\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.075\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.548402008318132\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007242560707819883\n",
      "        policy_loss: -0.021423516783022113\n",
      "        total_loss: 9.763248938898887\n",
      "        vf_explained_var: 0.017618633662500688\n",
      "        vf_loss: 9.784129291452388\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 6045.5\n",
      "  num_agent_steps_sampled: 28000\n",
      "  num_agent_steps_trained: 28000\n",
      "  num_env_steps_sampled: 28000\n",
      "  num_env_steps_trained: 28000\n",
      "iterations_since_restore: 7\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 28000\n",
      "num_agent_steps_trained: 28000\n",
      "num_env_steps_sampled: 28000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 28000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 19.121428571428574\n",
      "  ram_util_percent: 22.199999999999996\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08264466727505038\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04281754976063036\n",
      "  mean_inference_ms: 0.7302567254974389\n",
      "  mean_raw_obs_processing_ms: 0.21269640804002296\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.003958940505981445\n",
      "    StateBufferConnector_ms: 0.0030775070190429688\n",
      "    ViewRequirementAgentConnector_ms: 0.08319807052612305\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 207.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 207.58\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 14\n",
      "  hist_stats:\n",
      "    episode_lengths: [39, 28, 78, 28, 47, 36, 28, 18, 16, 87, 11, 96, 9, 21, 45, 42,\n",
      "      22, 24, 37, 55, 46, 114, 123, 120, 211, 95, 143, 123, 75, 166, 259, 151, 149,\n",
      "      220, 34, 178, 75, 107, 158, 359, 135, 18, 52, 262, 48, 38, 94, 149, 54, 117,\n",
      "      46, 237, 228, 262, 325, 62, 110, 194, 344, 121, 115, 448, 391, 500, 259, 107,\n",
      "      500, 182, 500, 87, 334, 220, 443, 389, 500, 470, 376, 500, 423, 500, 500, 500,\n",
      "      500, 500, 500, 500, 272, 232, 165, 289, 364, 169, 159, 417, 361, 295, 210, 164,\n",
      "      352, 496]\n",
      "    episode_reward: [39.0, 28.0, 78.0, 28.0, 47.0, 36.0, 28.0, 18.0, 16.0, 87.0, 11.0,\n",
      "      96.0, 9.0, 21.0, 45.0, 42.0, 22.0, 24.0, 37.0, 55.0, 46.0, 114.0, 123.0, 120.0,\n",
      "      211.0, 95.0, 143.0, 123.0, 75.0, 166.0, 259.0, 151.0, 149.0, 220.0, 34.0, 178.0,\n",
      "      75.0, 107.0, 158.0, 359.0, 135.0, 18.0, 52.0, 262.0, 48.0, 38.0, 94.0, 149.0,\n",
      "      54.0, 117.0, 46.0, 237.0, 228.0, 262.0, 325.0, 62.0, 110.0, 194.0, 344.0, 121.0,\n",
      "      115.0, 448.0, 391.0, 500.0, 259.0, 107.0, 500.0, 182.0, 500.0, 87.0, 334.0,\n",
      "      220.0, 443.0, 389.0, 500.0, 470.0, 376.0, 500.0, 423.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 272.0, 232.0, 165.0, 289.0, 364.0, 169.0, 159.0,\n",
      "      417.0, 361.0, 295.0, 210.0, 164.0, 352.0, 496.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08264466727505038\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04281754976063036\n",
      "    mean_inference_ms: 0.7302567254974389\n",
      "    mean_raw_obs_processing_ms: 0.21269640804002296\n",
      "time_since_restore: 70.01163697242737\n",
      "time_this_iter_s: 10.025898218154907\n",
      "time_total_s: 70.01163697242737\n",
      "timers:\n",
      "  learn_throughput: 705.845\n",
      "  learn_time_ms: 5666.966\n",
      "  load_throughput: 3032366.237\n",
      "  load_time_ms: 1.319\n",
      "  sample_time_ms: 4327.808\n",
      "  synch_weights_time_ms: 1.551\n",
      "  training_iteration_time_ms: 9998.227\n",
      "timestamp: 1683065240\n",
      "timesteps_total: 28000\n",
      "training_iteration: 7\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 32000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.003943443298339844\n",
      "  StateBufferConnector_ms: 0.0030727386474609375\n",
      "  ViewRequirementAgentConnector_ms: 0.08302831649780273\n",
      "counters:\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 32000\n",
      "  num_env_steps_trained: 32000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-07-30\n",
      "done: false\n",
      "episode_len_mean: 243.17\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 243.17\n",
      "episode_reward_min: 9.0\n",
      "episodes_this_iter: 10\n",
      "episodes_total: 368\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.075\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5168334071354199\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0037895150126784865\n",
      "        policy_loss: -0.020736992439275147\n",
      "        total_loss: 9.832915386076897\n",
      "        vf_explained_var: -0.17039252859289927\n",
      "        vf_loss: 9.853368159263365\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 6975.5\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 32000\n",
      "  num_env_steps_trained: 32000\n",
      "iterations_since_restore: 8\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 32000\n",
      "num_agent_steps_trained: 32000\n",
      "num_env_steps_sampled: 32000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 32000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 18.85\n",
      "  ram_util_percent: 22.199999999999996\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08285149005280255\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.042965265380313485\n",
      "  mean_inference_ms: 0.7323181531523079\n",
      "  mean_raw_obs_processing_ms: 0.21157438267261555\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.003943443298339844\n",
      "    StateBufferConnector_ms: 0.0030727386474609375\n",
      "    ViewRequirementAgentConnector_ms: 0.08302831649780273\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 243.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 243.17\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 10\n",
      "  hist_stats:\n",
      "    episode_lengths: [11, 96, 9, 21, 45, 42, 22, 24, 37, 55, 46, 114, 123, 120, 211,\n",
      "      95, 143, 123, 75, 166, 259, 151, 149, 220, 34, 178, 75, 107, 158, 359, 135,\n",
      "      18, 52, 262, 48, 38, 94, 149, 54, 117, 46, 237, 228, 262, 325, 62, 110, 194,\n",
      "      344, 121, 115, 448, 391, 500, 259, 107, 500, 182, 500, 87, 334, 220, 443, 389,\n",
      "      500, 470, 376, 500, 423, 500, 500, 500, 500, 500, 500, 500, 272, 232, 165, 289,\n",
      "      364, 169, 159, 417, 361, 295, 210, 164, 352, 496, 247, 500, 388, 500, 337, 500,\n",
      "      500, 109, 500, 383]\n",
      "    episode_reward: [11.0, 96.0, 9.0, 21.0, 45.0, 42.0, 22.0, 24.0, 37.0, 55.0, 46.0,\n",
      "      114.0, 123.0, 120.0, 211.0, 95.0, 143.0, 123.0, 75.0, 166.0, 259.0, 151.0, 149.0,\n",
      "      220.0, 34.0, 178.0, 75.0, 107.0, 158.0, 359.0, 135.0, 18.0, 52.0, 262.0, 48.0,\n",
      "      38.0, 94.0, 149.0, 54.0, 117.0, 46.0, 237.0, 228.0, 262.0, 325.0, 62.0, 110.0,\n",
      "      194.0, 344.0, 121.0, 115.0, 448.0, 391.0, 500.0, 259.0, 107.0, 500.0, 182.0,\n",
      "      500.0, 87.0, 334.0, 220.0, 443.0, 389.0, 500.0, 470.0, 376.0, 500.0, 423.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 272.0, 232.0, 165.0, 289.0,\n",
      "      364.0, 169.0, 159.0, 417.0, 361.0, 295.0, 210.0, 164.0, 352.0, 496.0, 247.0,\n",
      "      500.0, 388.0, 500.0, 337.0, 500.0, 500.0, 109.0, 500.0, 383.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08285149005280255\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.042965265380313485\n",
      "    mean_inference_ms: 0.7323181531523079\n",
      "    mean_raw_obs_processing_ms: 0.21157438267261555\n",
      "time_since_restore: 79.9353575706482\n",
      "time_this_iter_s: 9.923720598220825\n",
      "time_total_s: 79.9353575706482\n",
      "timers:\n",
      "  learn_throughput: 706.075\n",
      "  learn_time_ms: 5665.119\n",
      "  load_throughput: 3090580.455\n",
      "  load_time_ms: 1.294\n",
      "  sample_time_ms: 4320.011\n",
      "  synch_weights_time_ms: 1.538\n",
      "  training_iteration_time_ms: 9988.547\n",
      "timestamp: 1683065250\n",
      "timesteps_total: 32000\n",
      "training_iteration: 8\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 36000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.003919124603271484\n",
      "  StateBufferConnector_ms: 0.0030684471130371094\n",
      "  ViewRequirementAgentConnector_ms: 0.08340716361999512\n",
      "counters:\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_env_steps_sampled: 36000\n",
      "  num_env_steps_trained: 36000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-07-40\n",
      "done: false\n",
      "episode_len_mean: 277.08\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 277.08\n",
      "episode_reward_min: 18.0\n",
      "episodes_this_iter: 10\n",
      "episodes_total: 378\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.0375\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5365604642898806\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008343555059849777\n",
      "        policy_loss: -0.01903466879520365\n",
      "        total_loss: 9.808147974937192\n",
      "        vf_explained_var: 0.06202883630670527\n",
      "        vf_loss: 9.826869747202883\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 7905.5\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_env_steps_sampled: 36000\n",
      "  num_env_steps_trained: 36000\n",
      "iterations_since_restore: 9\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 36000\n",
      "num_agent_steps_trained: 36000\n",
      "num_env_steps_sampled: 36000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 36000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 14.107142857142858\n",
      "  ram_util_percent: 22.207142857142852\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0830657694827202\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04311788945326127\n",
      "  mean_inference_ms: 0.7344307374791549\n",
      "  mean_raw_obs_processing_ms: 0.2104012083047027\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.003919124603271484\n",
      "    StateBufferConnector_ms: 0.0030684471130371094\n",
      "    ViewRequirementAgentConnector_ms: 0.08340716361999512\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 277.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 277.08\n",
      "  episode_reward_min: 18.0\n",
      "  episodes_this_iter: 10\n",
      "  hist_stats:\n",
      "    episode_lengths: [46, 114, 123, 120, 211, 95, 143, 123, 75, 166, 259, 151, 149,\n",
      "      220, 34, 178, 75, 107, 158, 359, 135, 18, 52, 262, 48, 38, 94, 149, 54, 117,\n",
      "      46, 237, 228, 262, 325, 62, 110, 194, 344, 121, 115, 448, 391, 500, 259, 107,\n",
      "      500, 182, 500, 87, 334, 220, 443, 389, 500, 470, 376, 500, 423, 500, 500, 500,\n",
      "      500, 500, 500, 500, 272, 232, 165, 289, 364, 169, 159, 417, 361, 295, 210, 164,\n",
      "      352, 496, 247, 500, 388, 500, 337, 500, 500, 109, 500, 383, 344, 500, 500, 309,\n",
      "      243, 375, 384, 351, 402, 345]\n",
      "    episode_reward: [46.0, 114.0, 123.0, 120.0, 211.0, 95.0, 143.0, 123.0, 75.0, 166.0,\n",
      "      259.0, 151.0, 149.0, 220.0, 34.0, 178.0, 75.0, 107.0, 158.0, 359.0, 135.0, 18.0,\n",
      "      52.0, 262.0, 48.0, 38.0, 94.0, 149.0, 54.0, 117.0, 46.0, 237.0, 228.0, 262.0,\n",
      "      325.0, 62.0, 110.0, 194.0, 344.0, 121.0, 115.0, 448.0, 391.0, 500.0, 259.0,\n",
      "      107.0, 500.0, 182.0, 500.0, 87.0, 334.0, 220.0, 443.0, 389.0, 500.0, 470.0,\n",
      "      376.0, 500.0, 423.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 272.0,\n",
      "      232.0, 165.0, 289.0, 364.0, 169.0, 159.0, 417.0, 361.0, 295.0, 210.0, 164.0,\n",
      "      352.0, 496.0, 247.0, 500.0, 388.0, 500.0, 337.0, 500.0, 500.0, 109.0, 500.0,\n",
      "      383.0, 344.0, 500.0, 500.0, 309.0, 243.0, 375.0, 384.0, 351.0, 402.0, 345.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0830657694827202\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04311788945326127\n",
      "    mean_inference_ms: 0.7344307374791549\n",
      "    mean_raw_obs_processing_ms: 0.2104012083047027\n",
      "time_since_restore: 89.91517615318298\n",
      "time_this_iter_s: 9.97981858253479\n",
      "time_total_s: 89.91517615318298\n",
      "timers:\n",
      "  learn_throughput: 706.3\n",
      "  learn_time_ms: 5663.312\n",
      "  load_throughput: 3141866.123\n",
      "  load_time_ms: 1.273\n",
      "  sample_time_ms: 4320.56\n",
      "  synch_weights_time_ms: 1.546\n",
      "  training_iteration_time_ms: 9987.273\n",
      "timestamp: 1683065260\n",
      "timesteps_total: 36000\n",
      "training_iteration: 9\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 40000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.003996372222900391\n",
      "  StateBufferConnector_ms: 0.0030641555786132812\n",
      "  ViewRequirementAgentConnector_ms: 0.08330750465393066\n",
      "counters:\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 40000\n",
      "  num_env_steps_trained: 40000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-07-50\n",
      "done: false\n",
      "episode_len_mean: 306.85\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 306.85\n",
      "episode_reward_min: 18.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 386\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.0375\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5422690590222676\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005298625801301675\n",
      "        policy_loss: -0.024638419263866\n",
      "        total_loss: 9.872489812297205\n",
      "        vf_explained_var: -0.11921288736404911\n",
      "        vf_loss: 9.896929548119985\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 8835.5\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 40000\n",
      "  num_env_steps_trained: 40000\n",
      "iterations_since_restore: 10\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 40000\n",
      "num_agent_steps_trained: 40000\n",
      "num_env_steps_sampled: 40000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 40000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 20.72142857142857\n",
      "  ram_util_percent: 22.199999999999996\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08314335534325051\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.043172604813644655\n",
      "  mean_inference_ms: 0.7351419997098858\n",
      "  mean_raw_obs_processing_ms: 0.20965377136863403\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.003996372222900391\n",
      "    StateBufferConnector_ms: 0.0030641555786132812\n",
      "    ViewRequirementAgentConnector_ms: 0.08330750465393066\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 306.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 306.85\n",
      "  episode_reward_min: 18.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [75, 166, 259, 151, 149, 220, 34, 178, 75, 107, 158, 359, 135,\n",
      "      18, 52, 262, 48, 38, 94, 149, 54, 117, 46, 237, 228, 262, 325, 62, 110, 194,\n",
      "      344, 121, 115, 448, 391, 500, 259, 107, 500, 182, 500, 87, 334, 220, 443, 389,\n",
      "      500, 470, 376, 500, 423, 500, 500, 500, 500, 500, 500, 500, 272, 232, 165, 289,\n",
      "      364, 169, 159, 417, 361, 295, 210, 164, 352, 496, 247, 500, 388, 500, 337, 500,\n",
      "      500, 109, 500, 383, 344, 500, 500, 309, 243, 375, 384, 351, 402, 345, 500, 500,\n",
      "      500, 500, 500, 500, 452, 500]\n",
      "    episode_reward: [75.0, 166.0, 259.0, 151.0, 149.0, 220.0, 34.0, 178.0, 75.0, 107.0,\n",
      "      158.0, 359.0, 135.0, 18.0, 52.0, 262.0, 48.0, 38.0, 94.0, 149.0, 54.0, 117.0,\n",
      "      46.0, 237.0, 228.0, 262.0, 325.0, 62.0, 110.0, 194.0, 344.0, 121.0, 115.0, 448.0,\n",
      "      391.0, 500.0, 259.0, 107.0, 500.0, 182.0, 500.0, 87.0, 334.0, 220.0, 443.0,\n",
      "      389.0, 500.0, 470.0, 376.0, 500.0, 423.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 272.0, 232.0, 165.0, 289.0, 364.0, 169.0, 159.0, 417.0, 361.0,\n",
      "      295.0, 210.0, 164.0, 352.0, 496.0, 247.0, 500.0, 388.0, 500.0, 337.0, 500.0,\n",
      "      500.0, 109.0, 500.0, 383.0, 344.0, 500.0, 500.0, 309.0, 243.0, 375.0, 384.0,\n",
      "      351.0, 402.0, 345.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 452.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08314335534325051\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.043172604813644655\n",
      "    mean_inference_ms: 0.7351419997098858\n",
      "    mean_raw_obs_processing_ms: 0.20965377136863403\n",
      "time_since_restore: 99.88640880584717\n",
      "time_this_iter_s: 9.971232652664185\n",
      "time_total_s: 99.88640880584717\n",
      "timers:\n",
      "  learn_throughput: 706.369\n",
      "  learn_time_ms: 5662.766\n",
      "  load_throughput: 3145216.902\n",
      "  load_time_ms: 1.272\n",
      "  sample_time_ms: 4319.206\n",
      "  synch_weights_time_ms: 1.527\n",
      "  training_iteration_time_ms: 9985.361\n",
      "timestamp: 1683065270\n",
      "timesteps_total: 40000\n",
      "training_iteration: 10\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 44000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.004323720932006836\n",
      "  StateBufferConnector_ms: 0.0030469894409179688\n",
      "  ViewRequirementAgentConnector_ms: 0.08300352096557617\n",
      "counters:\n",
      "  num_agent_steps_sampled: 44000\n",
      "  num_agent_steps_trained: 44000\n",
      "  num_env_steps_sampled: 44000\n",
      "  num_env_steps_trained: 44000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-08-00\n",
      "done: false\n",
      "episode_len_mean: 334.53\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 334.53\n",
      "episode_reward_min: 18.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 394\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.0375\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5440442972285773\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005524351640413104\n",
      "        policy_loss: -0.02317990558922932\n",
      "        total_loss: 9.868548519380631\n",
      "        vf_explained_var: -0.22819982985014556\n",
      "        vf_loss: 9.891521265173472\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 9765.5\n",
      "  num_agent_steps_sampled: 44000\n",
      "  num_agent_steps_trained: 44000\n",
      "  num_env_steps_sampled: 44000\n",
      "  num_env_steps_trained: 44000\n",
      "iterations_since_restore: 11\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 44000\n",
      "num_agent_steps_trained: 44000\n",
      "num_env_steps_sampled: 44000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 44000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 20.959999999999997\n",
      "  ram_util_percent: 22.226666666666667\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08320527666256067\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04321434887335611\n",
      "  mean_inference_ms: 0.7357795177023095\n",
      "  mean_raw_obs_processing_ms: 0.20889308568936357\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004323720932006836\n",
      "    StateBufferConnector_ms: 0.0030469894409179688\n",
      "    ViewRequirementAgentConnector_ms: 0.08300352096557617\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 334.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 334.53\n",
      "  episode_reward_min: 18.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [75, 107, 158, 359, 135, 18, 52, 262, 48, 38, 94, 149, 54, 117,\n",
      "      46, 237, 228, 262, 325, 62, 110, 194, 344, 121, 115, 448, 391, 500, 259, 107,\n",
      "      500, 182, 500, 87, 334, 220, 443, 389, 500, 470, 376, 500, 423, 500, 500, 500,\n",
      "      500, 500, 500, 500, 272, 232, 165, 289, 364, 169, 159, 417, 361, 295, 210, 164,\n",
      "      352, 496, 247, 500, 388, 500, 337, 500, 500, 109, 500, 383, 344, 500, 500, 309,\n",
      "      243, 375, 384, 351, 402, 345, 500, 500, 500, 500, 500, 500, 452, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [75.0, 107.0, 158.0, 359.0, 135.0, 18.0, 52.0, 262.0, 48.0, 38.0,\n",
      "      94.0, 149.0, 54.0, 117.0, 46.0, 237.0, 228.0, 262.0, 325.0, 62.0, 110.0, 194.0,\n",
      "      344.0, 121.0, 115.0, 448.0, 391.0, 500.0, 259.0, 107.0, 500.0, 182.0, 500.0,\n",
      "      87.0, 334.0, 220.0, 443.0, 389.0, 500.0, 470.0, 376.0, 500.0, 423.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 272.0, 232.0, 165.0, 289.0, 364.0,\n",
      "      169.0, 159.0, 417.0, 361.0, 295.0, 210.0, 164.0, 352.0, 496.0, 247.0, 500.0,\n",
      "      388.0, 500.0, 337.0, 500.0, 500.0, 109.0, 500.0, 383.0, 344.0, 500.0, 500.0,\n",
      "      309.0, 243.0, 375.0, 384.0, 351.0, 402.0, 345.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 452.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08320527666256067\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04321434887335611\n",
      "    mean_inference_ms: 0.7357795177023095\n",
      "    mean_raw_obs_processing_ms: 0.20889308568936357\n",
      "time_since_restore: 109.89892578125\n",
      "time_this_iter_s: 10.012516975402832\n",
      "time_total_s: 109.89892578125\n",
      "timers:\n",
      "  learn_throughput: 702.133\n",
      "  learn_time_ms: 5696.925\n",
      "  load_throughput: 3093943.127\n",
      "  load_time_ms: 1.293\n",
      "  sample_time_ms: 4322.498\n",
      "  synch_weights_time_ms: 1.536\n",
      "  training_iteration_time_ms: 10022.841\n",
      "timestamp: 1683065280\n",
      "timesteps_total: 44000\n",
      "training_iteration: 11\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 48000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.00446629524230957\n",
      "  StateBufferConnector_ms: 0.003070831298828125\n",
      "  ViewRequirementAgentConnector_ms: 0.08321309089660645\n",
      "counters:\n",
      "  num_agent_steps_sampled: 48000\n",
      "  num_agent_steps_trained: 48000\n",
      "  num_env_steps_sampled: 48000\n",
      "  num_env_steps_trained: 48000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-08-10\n",
      "done: false\n",
      "episode_len_mean: 362.74\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 362.74\n",
      "episode_reward_min: 38.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 402\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.0375\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.515451465338789\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005964152985908893\n",
      "        policy_loss: -0.02173661750651175\n",
      "        total_loss: 9.864644111100063\n",
      "        vf_explained_var: -0.1472615131767847\n",
      "        vf_loss: 9.886157068642238\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 10695.5\n",
      "  num_agent_steps_sampled: 48000\n",
      "  num_agent_steps_trained: 48000\n",
      "  num_env_steps_sampled: 48000\n",
      "  num_env_steps_trained: 48000\n",
      "iterations_since_restore: 12\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 48000\n",
      "num_agent_steps_trained: 48000\n",
      "num_env_steps_sampled: 48000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 48000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 15.392857142857144\n",
      "  ram_util_percent: 22.292857142857144\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08326848848356345\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04325578797623366\n",
      "  mean_inference_ms: 0.7364060587875297\n",
      "  mean_raw_obs_processing_ms: 0.208105957410841\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.00446629524230957\n",
      "    StateBufferConnector_ms: 0.003070831298828125\n",
      "    ViewRequirementAgentConnector_ms: 0.08321309089660645\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 362.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 362.74\n",
      "  episode_reward_min: 38.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [48, 38, 94, 149, 54, 117, 46, 237, 228, 262, 325, 62, 110, 194,\n",
      "      344, 121, 115, 448, 391, 500, 259, 107, 500, 182, 500, 87, 334, 220, 443, 389,\n",
      "      500, 470, 376, 500, 423, 500, 500, 500, 500, 500, 500, 500, 272, 232, 165, 289,\n",
      "      364, 169, 159, 417, 361, 295, 210, 164, 352, 496, 247, 500, 388, 500, 337, 500,\n",
      "      500, 109, 500, 383, 344, 500, 500, 309, 243, 375, 384, 351, 402, 345, 500, 500,\n",
      "      500, 500, 500, 500, 452, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 487, 500]\n",
      "    episode_reward: [48.0, 38.0, 94.0, 149.0, 54.0, 117.0, 46.0, 237.0, 228.0, 262.0,\n",
      "      325.0, 62.0, 110.0, 194.0, 344.0, 121.0, 115.0, 448.0, 391.0, 500.0, 259.0,\n",
      "      107.0, 500.0, 182.0, 500.0, 87.0, 334.0, 220.0, 443.0, 389.0, 500.0, 470.0,\n",
      "      376.0, 500.0, 423.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 272.0,\n",
      "      232.0, 165.0, 289.0, 364.0, 169.0, 159.0, 417.0, 361.0, 295.0, 210.0, 164.0,\n",
      "      352.0, 496.0, 247.0, 500.0, 388.0, 500.0, 337.0, 500.0, 500.0, 109.0, 500.0,\n",
      "      383.0, 344.0, 500.0, 500.0, 309.0, 243.0, 375.0, 384.0, 351.0, 402.0, 345.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 452.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      487.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08326848848356345\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04325578797623366\n",
      "    mean_inference_ms: 0.7364060587875297\n",
      "    mean_raw_obs_processing_ms: 0.208105957410841\n",
      "time_since_restore: 119.90409302711487\n",
      "time_this_iter_s: 10.005167245864868\n",
      "time_total_s: 119.90409302711487\n",
      "timers:\n",
      "  learn_throughput: 705.664\n",
      "  learn_time_ms: 5668.424\n",
      "  load_throughput: 2924235.442\n",
      "  load_time_ms: 1.368\n",
      "  sample_time_ms: 4327.77\n",
      "  synch_weights_time_ms: 1.5\n",
      "  training_iteration_time_ms: 9999.653\n",
      "timestamp: 1683065290\n",
      "timesteps_total: 48000\n",
      "training_iteration: 12\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 52000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0047228336334228516\n",
      "  StateBufferConnector_ms: 0.0030531883239746094\n",
      "  ViewRequirementAgentConnector_ms: 0.08307027816772461\n",
      "counters:\n",
      "  num_agent_steps_sampled: 52000\n",
      "  num_agent_steps_trained: 52000\n",
      "  num_env_steps_sampled: 52000\n",
      "  num_env_steps_trained: 52000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-08-20\n",
      "done: false\n",
      "episode_len_mean: 394.91\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 394.91\n",
      "episode_reward_min: 62.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 410\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.0375\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5050477411798252\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004406780277484532\n",
      "        policy_loss: -0.023967331399520238\n",
      "        total_loss: 9.842570898609777\n",
      "        vf_explained_var: -0.21576491043131837\n",
      "        vf_loss: 9.866372979071832\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 11625.5\n",
      "  num_agent_steps_sampled: 52000\n",
      "  num_agent_steps_trained: 52000\n",
      "  num_env_steps_sampled: 52000\n",
      "  num_env_steps_trained: 52000\n",
      "iterations_since_restore: 13\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 52000\n",
      "num_agent_steps_trained: 52000\n",
      "num_env_steps_sampled: 52000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 52000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 20.313333333333333\n",
      "  ram_util_percent: 22.226666666666663\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0833296889808976\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04329930608729973\n",
      "  mean_inference_ms: 0.7370386250328747\n",
      "  mean_raw_obs_processing_ms: 0.2073281503056962\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0047228336334228516\n",
      "    StateBufferConnector_ms: 0.0030531883239746094\n",
      "    ViewRequirementAgentConnector_ms: 0.08307027816772461\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 394.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 394.91\n",
      "  episode_reward_min: 62.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [228, 262, 325, 62, 110, 194, 344, 121, 115, 448, 391, 500, 259,\n",
      "      107, 500, 182, 500, 87, 334, 220, 443, 389, 500, 470, 376, 500, 423, 500, 500,\n",
      "      500, 500, 500, 500, 500, 272, 232, 165, 289, 364, 169, 159, 417, 361, 295, 210,\n",
      "      164, 352, 496, 247, 500, 388, 500, 337, 500, 500, 109, 500, 383, 344, 500, 500,\n",
      "      309, 243, 375, 384, 351, 402, 345, 500, 500, 500, 500, 500, 500, 452, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 487, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [228.0, 262.0, 325.0, 62.0, 110.0, 194.0, 344.0, 121.0, 115.0,\n",
      "      448.0, 391.0, 500.0, 259.0, 107.0, 500.0, 182.0, 500.0, 87.0, 334.0, 220.0,\n",
      "      443.0, 389.0, 500.0, 470.0, 376.0, 500.0, 423.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 272.0, 232.0, 165.0, 289.0, 364.0, 169.0, 159.0, 417.0,\n",
      "      361.0, 295.0, 210.0, 164.0, 352.0, 496.0, 247.0, 500.0, 388.0, 500.0, 337.0,\n",
      "      500.0, 500.0, 109.0, 500.0, 383.0, 344.0, 500.0, 500.0, 309.0, 243.0, 375.0,\n",
      "      384.0, 351.0, 402.0, 345.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 452.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 487.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0833296889808976\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04329930608729973\n",
      "    mean_inference_ms: 0.7370386250328747\n",
      "    mean_raw_obs_processing_ms: 0.2073281503056962\n",
      "time_since_restore: 130.1347212791443\n",
      "time_this_iter_s: 10.230628252029419\n",
      "time_total_s: 130.1347212791443\n",
      "timers:\n",
      "  learn_throughput: 702.584\n",
      "  learn_time_ms: 5693.266\n",
      "  load_throughput: 2907864.67\n",
      "  load_time_ms: 1.376\n",
      "  sample_time_ms: 4316.867\n",
      "  synch_weights_time_ms: 1.559\n",
      "  training_iteration_time_ms: 10013.672\n",
      "timestamp: 1683065300\n",
      "timesteps_total: 52000\n",
      "training_iteration: 13\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 56000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0046956539154052734\n",
      "  StateBufferConnector_ms: 0.0030698776245117188\n",
      "  ViewRequirementAgentConnector_ms: 0.0833430290222168\n",
      "counters:\n",
      "  num_agent_steps_sampled: 56000\n",
      "  num_agent_steps_trained: 56000\n",
      "  num_env_steps_sampled: 56000\n",
      "  num_env_steps_trained: 56000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-08-30\n",
      "done: false\n",
      "episode_len_mean: 418.45\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 418.45\n",
      "episode_reward_min: 87.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 418\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.01875\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5123166523953919\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0030557614972358646\n",
      "        policy_loss: -0.021550273859212474\n",
      "        total_loss: 9.832885034622684\n",
      "        vf_explained_var: -0.2781346344178723\n",
      "        vf_loss: 9.854378016277026\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 12555.5\n",
      "  num_agent_steps_sampled: 56000\n",
      "  num_agent_steps_trained: 56000\n",
      "  num_env_steps_sampled: 56000\n",
      "  num_env_steps_trained: 56000\n",
      "iterations_since_restore: 14\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 56000\n",
      "num_agent_steps_trained: 56000\n",
      "num_env_steps_sampled: 56000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 56000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 17.87142857142857\n",
      "  ram_util_percent: 22.335714285714285\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08339257203943079\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04334691580307745\n",
      "  mean_inference_ms: 0.7377176957651318\n",
      "  mean_raw_obs_processing_ms: 0.20678542875156414\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0046956539154052734\n",
      "    StateBufferConnector_ms: 0.0030698776245117188\n",
      "    ViewRequirementAgentConnector_ms: 0.0833430290222168\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 418.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 418.45\n",
      "  episode_reward_min: 87.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [115, 448, 391, 500, 259, 107, 500, 182, 500, 87, 334, 220, 443,\n",
      "      389, 500, 470, 376, 500, 423, 500, 500, 500, 500, 500, 500, 500, 272, 232, 165,\n",
      "      289, 364, 169, 159, 417, 361, 295, 210, 164, 352, 496, 247, 500, 388, 500, 337,\n",
      "      500, 500, 109, 500, 383, 344, 500, 500, 309, 243, 375, 384, 351, 402, 345, 500,\n",
      "      500, 500, 500, 500, 500, 452, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 487, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [115.0, 448.0, 391.0, 500.0, 259.0, 107.0, 500.0, 182.0, 500.0,\n",
      "      87.0, 334.0, 220.0, 443.0, 389.0, 500.0, 470.0, 376.0, 500.0, 423.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 272.0, 232.0, 165.0, 289.0, 364.0,\n",
      "      169.0, 159.0, 417.0, 361.0, 295.0, 210.0, 164.0, 352.0, 496.0, 247.0, 500.0,\n",
      "      388.0, 500.0, 337.0, 500.0, 500.0, 109.0, 500.0, 383.0, 344.0, 500.0, 500.0,\n",
      "      309.0, 243.0, 375.0, 384.0, 351.0, 402.0, 345.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 452.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 487.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08339257203943079\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04334691580307745\n",
      "    mean_inference_ms: 0.7377176957651318\n",
      "    mean_raw_obs_processing_ms: 0.20678542875156414\n",
      "time_since_restore: 140.58015394210815\n",
      "time_this_iter_s: 10.445432662963867\n",
      "time_total_s: 140.58015394210815\n",
      "timers:\n",
      "  learn_throughput: 698.586\n",
      "  learn_time_ms: 5725.852\n",
      "  load_throughput: 3286299.459\n",
      "  load_time_ms: 1.217\n",
      "  sample_time_ms: 4332.101\n",
      "  synch_weights_time_ms: 1.551\n",
      "  training_iteration_time_ms: 10061.333\n",
      "timestamp: 1683065310\n",
      "timesteps_total: 56000\n",
      "training_iteration: 14\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 60000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.004656791687011719\n",
      "  StateBufferConnector_ms: 0.003070831298828125\n",
      "  ViewRequirementAgentConnector_ms: 0.0832509994506836\n",
      "counters:\n",
      "  num_agent_steps_sampled: 60000\n",
      "  num_agent_steps_trained: 60000\n",
      "  num_env_steps_sampled: 60000\n",
      "  num_env_steps_trained: 60000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-08-41\n",
      "done: false\n",
      "episode_len_mean: 433.43\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 433.43\n",
      "episode_reward_min: 87.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 426\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.009375\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5211066941420237\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006210529139341397\n",
      "        policy_loss: -0.02384610662377009\n",
      "        total_loss: 9.836125168749081\n",
      "        vf_explained_var: -0.18883572047756564\n",
      "        vf_loss: 9.859913072278422\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 13485.5\n",
      "  num_agent_steps_sampled: 60000\n",
      "  num_agent_steps_trained: 60000\n",
      "  num_env_steps_sampled: 60000\n",
      "  num_env_steps_trained: 60000\n",
      "iterations_since_restore: 15\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 60000\n",
      "num_agent_steps_trained: 60000\n",
      "num_env_steps_sampled: 60000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 60000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 20.113333333333337\n",
      "  ram_util_percent: 22.386666666666663\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08345880688628275\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.0433988991724069\n",
      "  mean_inference_ms: 0.7384313971223176\n",
      "  mean_raw_obs_processing_ms: 0.2062780622467915\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004656791687011719\n",
      "    StateBufferConnector_ms: 0.003070831298828125\n",
      "    ViewRequirementAgentConnector_ms: 0.0832509994506836\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 433.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 433.43\n",
      "  episode_reward_min: 87.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 87, 334, 220, 443, 389, 500, 470, 376, 500, 423, 500, 500,\n",
      "      500, 500, 500, 500, 500, 272, 232, 165, 289, 364, 169, 159, 417, 361, 295, 210,\n",
      "      164, 352, 496, 247, 500, 388, 500, 337, 500, 500, 109, 500, 383, 344, 500, 500,\n",
      "      309, 243, 375, 384, 351, 402, 345, 500, 500, 500, 500, 500, 500, 452, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 487, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [500.0, 87.0, 334.0, 220.0, 443.0, 389.0, 500.0, 470.0, 376.0,\n",
      "      500.0, 423.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 272.0, 232.0,\n",
      "      165.0, 289.0, 364.0, 169.0, 159.0, 417.0, 361.0, 295.0, 210.0, 164.0, 352.0,\n",
      "      496.0, 247.0, 500.0, 388.0, 500.0, 337.0, 500.0, 500.0, 109.0, 500.0, 383.0,\n",
      "      344.0, 500.0, 500.0, 309.0, 243.0, 375.0, 384.0, 351.0, 402.0, 345.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 452.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 487.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08345880688628275\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0433988991724069\n",
      "    mean_inference_ms: 0.7384313971223176\n",
      "    mean_raw_obs_processing_ms: 0.2062780622467915\n",
      "time_since_restore: 150.70239639282227\n",
      "time_this_iter_s: 10.122242450714111\n",
      "time_total_s: 150.70239639282227\n",
      "timers:\n",
      "  learn_throughput: 698.086\n",
      "  learn_time_ms: 5729.951\n",
      "  load_throughput: 3282634.37\n",
      "  load_time_ms: 1.219\n",
      "  sample_time_ms: 4338.29\n",
      "  synch_weights_time_ms: 1.564\n",
      "  training_iteration_time_ms: 10071.629\n",
      "timestamp: 1683065321\n",
      "timesteps_total: 60000\n",
      "training_iteration: 15\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 64000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.004610776901245117\n",
      "  StateBufferConnector_ms: 0.003040313720703125\n",
      "  ViewRequirementAgentConnector_ms: 0.08329010009765625\n",
      "counters:\n",
      "  num_agent_steps_sampled: 64000\n",
      "  num_agent_steps_trained: 64000\n",
      "  num_env_steps_sampled: 64000\n",
      "  num_env_steps_trained: 64000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-08-51\n",
      "done: false\n",
      "episode_len_mean: 444.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 444.0\n",
      "episode_reward_min: 109.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 434\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.009375\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5137198362940101\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00972723610651457\n",
      "        policy_loss: -0.024244950475391523\n",
      "        total_loss: 9.825091695272794\n",
      "        vf_explained_var: -0.16964905652948606\n",
      "        vf_loss: 9.849245442626296\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 14415.5\n",
      "  num_agent_steps_sampled: 64000\n",
      "  num_agent_steps_trained: 64000\n",
      "  num_env_steps_sampled: 64000\n",
      "  num_env_steps_trained: 64000\n",
      "iterations_since_restore: 16\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 64000\n",
      "num_agent_steps_trained: 64000\n",
      "num_env_steps_sampled: 64000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 64000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 19.035714285714285\n",
      "  ram_util_percent: 22.37857142857143\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08350933059952795\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04344219484528189\n",
      "  mean_inference_ms: 0.7389963679723962\n",
      "  mean_raw_obs_processing_ms: 0.20586791658562917\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004610776901245117\n",
      "    StateBufferConnector_ms: 0.003040313720703125\n",
      "    ViewRequirementAgentConnector_ms: 0.08329010009765625\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 444.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 444.0\n",
      "  episode_reward_min: 109.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [376, 500, 423, 500, 500, 500, 500, 500, 500, 500, 272, 232,\n",
      "      165, 289, 364, 169, 159, 417, 361, 295, 210, 164, 352, 496, 247, 500, 388, 500,\n",
      "      337, 500, 500, 109, 500, 383, 344, 500, 500, 309, 243, 375, 384, 351, 402, 345,\n",
      "      500, 500, 500, 500, 500, 500, 452, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 487, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [376.0, 500.0, 423.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 272.0, 232.0, 165.0, 289.0, 364.0, 169.0, 159.0, 417.0, 361.0, 295.0,\n",
      "      210.0, 164.0, 352.0, 496.0, 247.0, 500.0, 388.0, 500.0, 337.0, 500.0, 500.0,\n",
      "      109.0, 500.0, 383.0, 344.0, 500.0, 500.0, 309.0, 243.0, 375.0, 384.0, 351.0,\n",
      "      402.0, 345.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 452.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 487.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08350933059952795\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04344219484528189\n",
      "    mean_inference_ms: 0.7389963679723962\n",
      "    mean_raw_obs_processing_ms: 0.20586791658562917\n",
      "time_since_restore: 160.89138221740723\n",
      "time_this_iter_s: 10.188985824584961\n",
      "time_total_s: 160.89138221740723\n",
      "timers:\n",
      "  learn_throughput: 696.425\n",
      "  learn_time_ms: 5743.62\n",
      "  load_throughput: 3303772.203\n",
      "  load_time_ms: 1.211\n",
      "  sample_time_ms: 4340.67\n",
      "  synch_weights_time_ms: 1.567\n",
      "  training_iteration_time_ms: 10087.68\n",
      "timestamp: 1683065331\n",
      "timesteps_total: 64000\n",
      "training_iteration: 16\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 68000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.00481867790222168\n",
      "  StateBufferConnector_ms: 0.003078460693359375\n",
      "  ViewRequirementAgentConnector_ms: 0.08323454856872559\n",
      "counters:\n",
      "  num_agent_steps_sampled: 68000\n",
      "  num_agent_steps_trained: 68000\n",
      "  num_env_steps_sampled: 68000\n",
      "  num_env_steps_trained: 68000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-09-01\n",
      "done: false\n",
      "episode_len_mean: 440.79\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 440.79\n",
      "episode_reward_min: 109.0\n",
      "episodes_this_iter: 9\n",
      "episodes_total: 443\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.009375\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.49692229612540173\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006426602207561475\n",
      "        policy_loss: -0.01816479779819968\n",
      "        total_loss: 9.79227439716298\n",
      "        vf_explained_var: -0.18236118183341077\n",
      "        vf_loss: 9.810378966792937\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 15345.5\n",
      "  num_agent_steps_sampled: 68000\n",
      "  num_agent_steps_trained: 68000\n",
      "  num_env_steps_sampled: 68000\n",
      "  num_env_steps_trained: 68000\n",
      "iterations_since_restore: 17\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 68000\n",
      "num_agent_steps_trained: 68000\n",
      "num_env_steps_sampled: 68000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 68000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 33.333333333333336\n",
      "  ram_util_percent: 22.38\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08355985788402631\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.0434882855349054\n",
      "  mean_inference_ms: 0.7395818854844067\n",
      "  mean_raw_obs_processing_ms: 0.20551176646375657\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.00481867790222168\n",
      "    StateBufferConnector_ms: 0.003078460693359375\n",
      "    ViewRequirementAgentConnector_ms: 0.08323454856872559\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 440.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 440.79\n",
      "  episode_reward_min: 109.0\n",
      "  episodes_this_iter: 9\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 272, 232, 165, 289, 364, 169, 159, 417, 361, 295, 210,\n",
      "      164, 352, 496, 247, 500, 388, 500, 337, 500, 500, 109, 500, 383, 344, 500, 500,\n",
      "      309, 243, 375, 384, 351, 402, 345, 500, 500, 500, 500, 500, 500, 452, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 487, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 265, 500, 500, 500, 500, 213, 500]\n",
      "    episode_reward: [500.0, 272.0, 232.0, 165.0, 289.0, 364.0, 169.0, 159.0, 417.0,\n",
      "      361.0, 295.0, 210.0, 164.0, 352.0, 496.0, 247.0, 500.0, 388.0, 500.0, 337.0,\n",
      "      500.0, 500.0, 109.0, 500.0, 383.0, 344.0, 500.0, 500.0, 309.0, 243.0, 375.0,\n",
      "      384.0, 351.0, 402.0, 345.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 452.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 487.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 265.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 213.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08355985788402631\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0434882855349054\n",
      "    mean_inference_ms: 0.7395818854844067\n",
      "    mean_raw_obs_processing_ms: 0.20551176646375657\n",
      "time_since_restore: 171.03596544265747\n",
      "time_this_iter_s: 10.144583225250244\n",
      "time_total_s: 171.03596544265747\n",
      "timers:\n",
      "  learn_throughput: 695.661\n",
      "  learn_time_ms: 5749.924\n",
      "  load_throughput: 3101607.631\n",
      "  load_time_ms: 1.29\n",
      "  sample_time_ms: 4346.19\n",
      "  synch_weights_time_ms: 1.552\n",
      "  training_iteration_time_ms: 10099.565\n",
      "timestamp: 1683065341\n",
      "timesteps_total: 68000\n",
      "training_iteration: 17\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 72000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.004749298095703125\n",
      "  StateBufferConnector_ms: 0.003093719482421875\n",
      "  ViewRequirementAgentConnector_ms: 0.08336019515991211\n",
      "counters:\n",
      "  num_agent_steps_sampled: 72000\n",
      "  num_agent_steps_trained: 72000\n",
      "  num_env_steps_sampled: 72000\n",
      "  num_env_steps_trained: 72000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-09-11\n",
      "done: false\n",
      "episode_len_mean: 459.29\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 459.29\n",
      "episode_reward_min: 109.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 451\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.009375\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.47781673461519264\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006462121025356647\n",
      "        policy_loss: -0.023302805255497657\n",
      "        total_loss: 9.795165055797947\n",
      "        vf_explained_var: -0.09386193495924755\n",
      "        vf_loss: 9.818407289443478\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 16275.5\n",
      "  num_agent_steps_sampled: 72000\n",
      "  num_agent_steps_trained: 72000\n",
      "  num_env_steps_sampled: 72000\n",
      "  num_env_steps_trained: 72000\n",
      "iterations_since_restore: 18\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 72000\n",
      "num_agent_steps_trained: 72000\n",
      "num_env_steps_sampled: 72000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 72000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 20.235714285714288\n",
      "  ram_util_percent: 22.34285714285715\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.083595906174875\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04352534650431919\n",
      "  mean_inference_ms: 0.7400176330880777\n",
      "  mean_raw_obs_processing_ms: 0.2052590230180442\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004749298095703125\n",
      "    StateBufferConnector_ms: 0.003093719482421875\n",
      "    ViewRequirementAgentConnector_ms: 0.08336019515991211\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 459.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 459.29\n",
      "  episode_reward_min: 109.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [417, 361, 295, 210, 164, 352, 496, 247, 500, 388, 500, 337,\n",
      "      500, 500, 109, 500, 383, 344, 500, 500, 309, 243, 375, 384, 351, 402, 345, 500,\n",
      "      500, 500, 500, 500, 500, 452, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 487, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 265, 500, 500, 500, 500, 213, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [417.0, 361.0, 295.0, 210.0, 164.0, 352.0, 496.0, 247.0, 500.0,\n",
      "      388.0, 500.0, 337.0, 500.0, 500.0, 109.0, 500.0, 383.0, 344.0, 500.0, 500.0,\n",
      "      309.0, 243.0, 375.0, 384.0, 351.0, 402.0, 345.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 452.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 487.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 265.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 213.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.083595906174875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04352534650431919\n",
      "    mean_inference_ms: 0.7400176330880777\n",
      "    mean_raw_obs_processing_ms: 0.2052590230180442\n",
      "time_since_restore: 181.09049320220947\n",
      "time_this_iter_s: 10.054527759552002\n",
      "time_total_s: 181.09049320220947\n",
      "timers:\n",
      "  learn_throughput: 695.249\n",
      "  learn_time_ms: 5753.335\n",
      "  load_throughput: 3104821.971\n",
      "  load_time_ms: 1.288\n",
      "  sample_time_ms: 4355.879\n",
      "  synch_weights_time_ms: 1.548\n",
      "  training_iteration_time_ms: 10112.664\n",
      "timestamp: 1683065351\n",
      "timesteps_total: 72000\n",
      "training_iteration: 18\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 76000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.00464177131652832\n",
      "  StateBufferConnector_ms: 0.0030798912048339844\n",
      "  ViewRequirementAgentConnector_ms: 0.0836036205291748\n",
      "counters:\n",
      "  num_agent_steps_sampled: 76000\n",
      "  num_agent_steps_trained: 76000\n",
      "  num_env_steps_sampled: 76000\n",
      "  num_env_steps_trained: 76000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-09-21\n",
      "done: false\n",
      "episode_len_mean: 473.87\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 473.87\n",
      "episode_reward_min: 109.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 459\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.009375\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.4802452430609734\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007212385148913203\n",
      "        policy_loss: -0.02213030752215174\n",
      "        total_loss: 9.781204962986772\n",
      "        vf_explained_var: -0.057005350179569696\n",
      "        vf_loss: 9.803267660448627\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 17205.5\n",
      "  num_agent_steps_sampled: 76000\n",
      "  num_agent_steps_trained: 76000\n",
      "  num_env_steps_sampled: 76000\n",
      "  num_env_steps_trained: 76000\n",
      "iterations_since_restore: 19\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 76000\n",
      "num_agent_steps_trained: 76000\n",
      "num_env_steps_sampled: 76000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 76000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 24.446666666666665\n",
      "  ram_util_percent: 22.300000000000004\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08363528890617562\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.043565513949488446\n",
      "  mean_inference_ms: 0.740501896214128\n",
      "  mean_raw_obs_processing_ms: 0.20502310493985335\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.00464177131652832\n",
      "    StateBufferConnector_ms: 0.0030798912048339844\n",
      "    ViewRequirementAgentConnector_ms: 0.0836036205291748\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 473.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 473.87\n",
      "  episode_reward_min: 109.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 388, 500, 337, 500, 500, 109, 500, 383, 344, 500, 500,\n",
      "      309, 243, 375, 384, 351, 402, 345, 500, 500, 500, 500, 500, 500, 452, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 487, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 265, 500, 500, 500, 500, 213, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [500.0, 388.0, 500.0, 337.0, 500.0, 500.0, 109.0, 500.0, 383.0,\n",
      "      344.0, 500.0, 500.0, 309.0, 243.0, 375.0, 384.0, 351.0, 402.0, 345.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 452.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 487.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 265.0, 500.0, 500.0, 500.0, 500.0, 213.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08363528890617562\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.043565513949488446\n",
      "    mean_inference_ms: 0.740501896214128\n",
      "    mean_raw_obs_processing_ms: 0.20502310493985335\n",
      "time_since_restore: 191.37156176567078\n",
      "time_this_iter_s: 10.281068563461304\n",
      "time_total_s: 191.37156176567078\n",
      "timers:\n",
      "  learn_throughput: 692.494\n",
      "  learn_time_ms: 5776.221\n",
      "  load_throughput: 3120645.809\n",
      "  load_time_ms: 1.282\n",
      "  sample_time_ms: 4363.12\n",
      "  synch_weights_time_ms: 1.55\n",
      "  training_iteration_time_ms: 10142.795\n",
      "timestamp: 1683065361\n",
      "timesteps_total: 76000\n",
      "training_iteration: 19\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 80000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.004662752151489258\n",
      "  StateBufferConnector_ms: 0.0030786991119384766\n",
      "  ViewRequirementAgentConnector_ms: 0.08454632759094238\n",
      "counters:\n",
      "  num_agent_steps_sampled: 80000\n",
      "  num_agent_steps_trained: 80000\n",
      "  num_env_steps_sampled: 80000\n",
      "  num_env_steps_trained: 80000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-09-32\n",
      "done: false\n",
      "episode_len_mean: 480.16\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 480.16\n",
      "episode_reward_min: 213.0\n",
      "episodes_this_iter: 9\n",
      "episodes_total: 468\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.009375\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.46210365987593127\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005299032900873446\n",
      "        policy_loss: -0.015549917342842267\n",
      "        total_loss: 9.740207421907815\n",
      "        vf_explained_var: -0.10679098136963383\n",
      "        vf_loss: 9.755707619779853\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 18135.5\n",
      "  num_agent_steps_sampled: 80000\n",
      "  num_agent_steps_trained: 80000\n",
      "  num_env_steps_sampled: 80000\n",
      "  num_env_steps_trained: 80000\n",
      "iterations_since_restore: 20\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 80000\n",
      "num_agent_steps_trained: 80000\n",
      "num_env_steps_sampled: 80000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 80000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 27.779999999999998\n",
      "  ram_util_percent: 22.37333333333333\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08370908863379141\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04363590728725473\n",
      "  mean_inference_ms: 0.7413636544167538\n",
      "  mean_raw_obs_processing_ms: 0.2048783188003112\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004662752151489258\n",
      "    StateBufferConnector_ms: 0.0030786991119384766\n",
      "    ViewRequirementAgentConnector_ms: 0.08454632759094238\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 480.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 480.16\n",
      "  episode_reward_min: 213.0\n",
      "  episodes_this_iter: 9\n",
      "  hist_stats:\n",
      "    episode_lengths: [344, 500, 500, 309, 243, 375, 384, 351, 402, 345, 500, 500,\n",
      "      500, 500, 500, 500, 452, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 487, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 265, 500, 500, 500, 500, 213, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      471, 500, 500, 375, 500, 500, 500, 500]\n",
      "    episode_reward: [344.0, 500.0, 500.0, 309.0, 243.0, 375.0, 384.0, 351.0, 402.0,\n",
      "      345.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 452.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 487.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 265.0, 500.0, 500.0, 500.0, 500.0, 213.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 471.0, 500.0, 500.0, 375.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08370908863379141\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04363590728725473\n",
      "    mean_inference_ms: 0.7413636544167538\n",
      "    mean_raw_obs_processing_ms: 0.2048783188003112\n",
      "time_since_restore: 201.8441641330719\n",
      "time_this_iter_s: 10.472602367401123\n",
      "time_total_s: 201.8441641330719\n",
      "timers:\n",
      "  learn_throughput: 690.582\n",
      "  learn_time_ms: 5792.217\n",
      "  load_throughput: 3166886.762\n",
      "  load_time_ms: 1.263\n",
      "  sample_time_ms: 4397.274\n",
      "  synch_weights_time_ms: 1.578\n",
      "  training_iteration_time_ms: 10192.945\n",
      "timestamp: 1683065372\n",
      "timesteps_total: 80000\n",
      "training_iteration: 20\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 84000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.004717350006103516\n",
      "  StateBufferConnector_ms: 0.0030579566955566406\n",
      "  ViewRequirementAgentConnector_ms: 0.08463716506958008\n",
      "counters:\n",
      "  num_agent_steps_sampled: 84000\n",
      "  num_agent_steps_trained: 84000\n",
      "  num_env_steps_sampled: 84000\n",
      "  num_env_steps_trained: 84000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-09-42\n",
      "done: false\n",
      "episode_len_mean: 490.1\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 490.1\n",
      "episode_reward_min: 213.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 476\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.009375\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.4822179646581732\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0035467890338774346\n",
      "        policy_loss: -0.02154907653689064\n",
      "        total_loss: 9.734897794005692\n",
      "        vf_explained_var: -0.10646694193604173\n",
      "        vf_loss: 9.756413628978114\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 19065.5\n",
      "  num_agent_steps_sampled: 84000\n",
      "  num_agent_steps_trained: 84000\n",
      "  num_env_steps_sampled: 84000\n",
      "  num_env_steps_trained: 84000\n",
      "iterations_since_restore: 21\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 84000\n",
      "num_agent_steps_trained: 84000\n",
      "num_env_steps_sampled: 84000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 84000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 15.471428571428572\n",
      "  ram_util_percent: 22.4\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08376482240998794\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.043694135596315284\n",
      "  mean_inference_ms: 0.742073479298287\n",
      "  mean_raw_obs_processing_ms: 0.2047767021377477\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004717350006103516\n",
      "    StateBufferConnector_ms: 0.0030579566955566406\n",
      "    ViewRequirementAgentConnector_ms: 0.08463716506958008\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 490.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 490.1\n",
      "  episode_reward_min: 213.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [402, 345, 500, 500, 500, 500, 500, 500, 452, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 487, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      265, 500, 500, 500, 500, 213, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 471, 500, 500, 375, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [402.0, 345.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 452.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 487.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 265.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 213.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 471.0, 500.0,\n",
      "      500.0, 375.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08376482240998794\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.043694135596315284\n",
      "    mean_inference_ms: 0.742073479298287\n",
      "    mean_raw_obs_processing_ms: 0.2047767021377477\n",
      "time_since_restore: 211.86133933067322\n",
      "time_this_iter_s: 10.017175197601318\n",
      "time_total_s: 211.86133933067322\n",
      "timers:\n",
      "  learn_throughput: 690.464\n",
      "  learn_time_ms: 5793.204\n",
      "  load_throughput: 3183472.04\n",
      "  load_time_ms: 1.256\n",
      "  sample_time_ms: 4396.808\n",
      "  synch_weights_time_ms: 1.572\n",
      "  training_iteration_time_ms: 10193.462\n",
      "timestamp: 1683065382\n",
      "timesteps_total: 84000\n",
      "training_iteration: 21\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 88000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.004930734634399414\n",
      "  StateBufferConnector_ms: 0.0030853748321533203\n",
      "  ViewRequirementAgentConnector_ms: 0.08473086357116699\n",
      "counters:\n",
      "  num_agent_steps_sampled: 88000\n",
      "  num_agent_steps_trained: 88000\n",
      "  num_env_steps_sampled: 88000\n",
      "  num_env_steps_trained: 88000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-09-52\n",
      "done: false\n",
      "episode_len_mean: 492.63\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 492.63\n",
      "episode_reward_min: 213.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 484\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.0046875\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.4752879994851287\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005675940635731173\n",
      "        policy_loss: -0.022935861054687732\n",
      "        total_loss: 9.701544213038618\n",
      "        vf_explained_var: -0.10933517807273455\n",
      "        vf_loss: 9.724453469758393\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 19995.5\n",
      "  num_agent_steps_sampled: 88000\n",
      "  num_agent_steps_trained: 88000\n",
      "  num_env_steps_sampled: 88000\n",
      "  num_env_steps_trained: 88000\n",
      "iterations_since_restore: 22\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 88000\n",
      "num_agent_steps_trained: 88000\n",
      "num_env_steps_sampled: 88000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 88000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 29.764285714285712\n",
      "  ram_util_percent: 22.4\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08381983211708714\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04375216135080206\n",
      "  mean_inference_ms: 0.7427697657658965\n",
      "  mean_raw_obs_processing_ms: 0.2046988054080826\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004930734634399414\n",
      "    StateBufferConnector_ms: 0.0030853748321533203\n",
      "    ViewRequirementAgentConnector_ms: 0.08473086357116699\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 492.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 492.63\n",
      "  episode_reward_min: 213.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [452, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 487, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 265, 500, 500, 500, 500, 213, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      471, 500, 500, 375, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [452.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 487.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 265.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 213.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 471.0, 500.0, 500.0, 375.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08381983211708714\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04375216135080206\n",
      "    mean_inference_ms: 0.7427697657658965\n",
      "    mean_raw_obs_processing_ms: 0.2046988054080826\n",
      "time_since_restore: 221.95306277275085\n",
      "time_this_iter_s: 10.091723442077637\n",
      "time_total_s: 221.95306277275085\n",
      "timers:\n",
      "  learn_throughput: 689.985\n",
      "  learn_time_ms: 5797.225\n",
      "  load_throughput: 3436756.867\n",
      "  load_time_ms: 1.164\n",
      "  sample_time_ms: 4401.542\n",
      "  synch_weights_time_ms: 1.563\n",
      "  training_iteration_time_ms: 10202.118\n",
      "timestamp: 1683065392\n",
      "timesteps_total: 88000\n",
      "training_iteration: 22\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 92000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.004730701446533203\n",
      "  StateBufferConnector_ms: 0.003099203109741211\n",
      "  ViewRequirementAgentConnector_ms: 0.08514642715454102\n",
      "counters:\n",
      "  num_agent_steps_sampled: 92000\n",
      "  num_agent_steps_trained: 92000\n",
      "  num_env_steps_sampled: 92000\n",
      "  num_env_steps_trained: 92000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-10-02\n",
      "done: false\n",
      "episode_len_mean: 493.11\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 493.11\n",
      "episode_reward_min: 213.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 492\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.0046875\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.43963472407351256\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.003774975608795754\n",
      "        policy_loss: -0.022856924428494386\n",
      "        total_loss: 9.655828335464642\n",
      "        vf_explained_var: -0.10137833363266402\n",
      "        vf_loss: 9.678667569929553\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 20925.5\n",
      "  num_agent_steps_sampled: 92000\n",
      "  num_agent_steps_trained: 92000\n",
      "  num_env_steps_sampled: 92000\n",
      "  num_env_steps_trained: 92000\n",
      "iterations_since_restore: 23\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 92000\n",
      "num_agent_steps_trained: 92000\n",
      "num_env_steps_sampled: 92000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 92000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 21.113333333333333\n",
      "  ram_util_percent: 22.399999999999995\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08387659388767216\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.043813056459760434\n",
      "  mean_inference_ms: 0.7434300212650956\n",
      "  mean_raw_obs_processing_ms: 0.204661208498677\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004730701446533203\n",
      "    StateBufferConnector_ms: 0.003099203109741211\n",
      "    ViewRequirementAgentConnector_ms: 0.08514642715454102\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 493.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 493.11\n",
      "  episode_reward_min: 213.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 487, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      265, 500, 500, 500, 500, 213, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 471, 500, 500, 375, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 487.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 265.0, 500.0, 500.0, 500.0, 500.0, 213.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 471.0, 500.0, 500.0, 375.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08387659388767216\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.043813056459760434\n",
      "    mean_inference_ms: 0.7434300212650956\n",
      "    mean_raw_obs_processing_ms: 0.204661208498677\n",
      "time_since_restore: 232.09261798858643\n",
      "time_this_iter_s: 10.139555215835571\n",
      "time_total_s: 232.09261798858643\n",
      "timers:\n",
      "  learn_throughput: 691.711\n",
      "  learn_time_ms: 5782.76\n",
      "  load_throughput: 3398054.807\n",
      "  load_time_ms: 1.177\n",
      "  sample_time_ms: 4406.953\n",
      "  synch_weights_time_ms: 1.497\n",
      "  training_iteration_time_ms: 10193.007\n",
      "timestamp: 1683065402\n",
      "timesteps_total: 92000\n",
      "training_iteration: 23\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 96000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.004550457000732422\n",
      "  StateBufferConnector_ms: 0.003126859664916992\n",
      "  ViewRequirementAgentConnector_ms: 0.08516168594360352\n",
      "counters:\n",
      "  num_agent_steps_sampled: 96000\n",
      "  num_agent_steps_trained: 96000\n",
      "  num_env_steps_sampled: 96000\n",
      "  num_env_steps_trained: 96000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-10-12\n",
      "done: false\n",
      "episode_len_mean: 493.11\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 493.11\n",
      "episode_reward_min: 213.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 500\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.00234375\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.45306421636894184\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004020327597884159\n",
      "        policy_loss: -0.02102998465059265\n",
      "        total_loss: 9.587468618987709\n",
      "        vf_explained_var: -0.09446497091683008\n",
      "        vf_loss: 9.608489184225759\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 21855.5\n",
      "  num_agent_steps_sampled: 96000\n",
      "  num_agent_steps_trained: 96000\n",
      "  num_env_steps_sampled: 96000\n",
      "  num_env_steps_trained: 96000\n",
      "iterations_since_restore: 24\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 96000\n",
      "num_agent_steps_trained: 96000\n",
      "num_env_steps_sampled: 96000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 96000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 22.607142857142858\n",
      "  ram_util_percent: 22.4\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0839309962382206\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04387668060433298\n",
      "  mean_inference_ms: 0.7440895440806624\n",
      "  mean_raw_obs_processing_ms: 0.20465187406119714\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004550457000732422\n",
      "    StateBufferConnector_ms: 0.003126859664916992\n",
      "    ViewRequirementAgentConnector_ms: 0.08516168594360352\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 493.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 493.11\n",
      "  episode_reward_min: 213.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [487, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 265, 500, 500, 500, 500, 213, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      471, 500, 500, 375, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [487.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 265.0, 500.0, 500.0, 500.0, 500.0, 213.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 471.0, 500.0, 500.0, 375.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0839309962382206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04387668060433298\n",
      "    mean_inference_ms: 0.7440895440806624\n",
      "    mean_raw_obs_processing_ms: 0.20465187406119714\n",
      "time_since_restore: 242.33985090255737\n",
      "time_this_iter_s: 10.247232913970947\n",
      "time_total_s: 242.33985090255737\n",
      "timers:\n",
      "  learn_throughput: 693.039\n",
      "  learn_time_ms: 5771.681\n",
      "  load_throughput: 3389815.934\n",
      "  load_time_ms: 1.18\n",
      "  sample_time_ms: 4398.24\n",
      "  synch_weights_time_ms: 1.48\n",
      "  training_iteration_time_ms: 10173.197\n",
      "timestamp: 1683065412\n",
      "timesteps_total: 96000\n",
      "training_iteration: 24\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 100000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.004273176193237305\n",
      "  StateBufferConnector_ms: 0.003099679946899414\n",
      "  ViewRequirementAgentConnector_ms: 0.08498096466064453\n",
      "counters:\n",
      "  num_agent_steps_sampled: 100000\n",
      "  num_agent_steps_trained: 100000\n",
      "  num_env_steps_sampled: 100000\n",
      "  num_env_steps_trained: 100000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-10-22\n",
      "done: false\n",
      "episode_len_mean: 493.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 493.24\n",
      "episode_reward_min: 213.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 508\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.001171875\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.43318430406431996\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007048182134055978\n",
      "        policy_loss: -0.023163208012939782\n",
      "        total_loss: 9.439900077799315\n",
      "        vf_explained_var: -0.0478948409839343\n",
      "        vf_loss: 9.463055047168526\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 22785.5\n",
      "  num_agent_steps_sampled: 100000\n",
      "  num_agent_steps_trained: 100000\n",
      "  num_env_steps_sampled: 100000\n",
      "  num_env_steps_trained: 100000\n",
      "iterations_since_restore: 25\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 100000\n",
      "num_agent_steps_trained: 100000\n",
      "num_env_steps_sampled: 100000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 100000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 20.0\n",
      "  ram_util_percent: 22.399999999999995\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08397940837480633\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04393487680012191\n",
      "  mean_inference_ms: 0.7446888439843365\n",
      "  mean_raw_obs_processing_ms: 0.20464726609405312\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004273176193237305\n",
      "    StateBufferConnector_ms: 0.003099679946899414\n",
      "    ViewRequirementAgentConnector_ms: 0.08498096466064453\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 493.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 493.24\n",
      "  episode_reward_min: 213.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      265, 500, 500, 500, 500, 213, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 471, 500, 500, 375, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 265.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 213.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 471.0,\n",
      "      500.0, 500.0, 375.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08397940837480633\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04393487680012191\n",
      "    mean_inference_ms: 0.7446888439843365\n",
      "    mean_raw_obs_processing_ms: 0.20464726609405312\n",
      "time_since_restore: 252.29765725135803\n",
      "time_this_iter_s: 9.95780634880066\n",
      "time_total_s: 252.29765725135803\n",
      "timers:\n",
      "  learn_throughput: 693.197\n",
      "  learn_time_ms: 5770.365\n",
      "  load_throughput: 3377260.302\n",
      "  load_time_ms: 1.184\n",
      "  sample_time_ms: 4383.077\n",
      "  synch_weights_time_ms: 1.518\n",
      "  training_iteration_time_ms: 10156.756\n",
      "timestamp: 1683065422\n",
      "timesteps_total: 100000\n",
      "training_iteration: 25\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 104000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.004193782806396484\n",
      "  StateBufferConnector_ms: 0.0030977725982666016\n",
      "  ViewRequirementAgentConnector_ms: 0.0848076343536377\n",
      "counters:\n",
      "  num_agent_steps_sampled: 104000\n",
      "  num_agent_steps_trained: 104000\n",
      "  num_env_steps_sampled: 104000\n",
      "  num_env_steps_trained: 104000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-10-32\n",
      "done: false\n",
      "episode_len_mean: 493.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 493.24\n",
      "episode_reward_min: 213.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 516\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.001171875\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.4409882117343205\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0023031865646728714\n",
      "        policy_loss: -0.021762822944951313\n",
      "        total_loss: 9.024154809213453\n",
      "        vf_explained_var: -0.02569115892533333\n",
      "        vf_loss: 9.045914936834766\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 23715.5\n",
      "  num_agent_steps_sampled: 104000\n",
      "  num_agent_steps_trained: 104000\n",
      "  num_env_steps_sampled: 104000\n",
      "  num_env_steps_trained: 104000\n",
      "iterations_since_restore: 26\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 104000\n",
      "num_agent_steps_trained: 104000\n",
      "num_env_steps_sampled: 104000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 104000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 20.521428571428572\n",
      "  ram_util_percent: 22.39285714285714\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08401511301778093\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.043981694485561905\n",
      "  mean_inference_ms: 0.7451761924414387\n",
      "  mean_raw_obs_processing_ms: 0.20463014045842948\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004193782806396484\n",
      "    StateBufferConnector_ms: 0.0030977725982666016\n",
      "    ViewRequirementAgentConnector_ms: 0.0848076343536377\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 493.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 493.24\n",
      "  episode_reward_min: 213.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 265, 500, 500, 500, 500, 213, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      471, 500, 500, 375, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      265.0, 500.0, 500.0, 500.0, 500.0, 213.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 471.0, 500.0, 500.0, 375.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08401511301778093\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.043981694485561905\n",
      "    mean_inference_ms: 0.7451761924414387\n",
      "    mean_raw_obs_processing_ms: 0.20463014045842948\n",
      "time_since_restore: 262.33487462997437\n",
      "time_this_iter_s: 10.037217378616333\n",
      "time_total_s: 262.33487462997437\n",
      "timers:\n",
      "  learn_throughput: 694.823\n",
      "  learn_time_ms: 5756.864\n",
      "  load_throughput: 3347008.738\n",
      "  load_time_ms: 1.195\n",
      "  sample_time_ms: 4381.392\n",
      "  synch_weights_time_ms: 1.527\n",
      "  training_iteration_time_ms: 10141.588\n",
      "timestamp: 1683065432\n",
      "timesteps_total: 104000\n",
      "training_iteration: 26\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 108000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.004178047180175781\n",
      "  StateBufferConnector_ms: 0.003106832504272461\n",
      "  ViewRequirementAgentConnector_ms: 0.08489322662353516\n",
      "counters:\n",
      "  num_agent_steps_sampled: 108000\n",
      "  num_agent_steps_trained: 108000\n",
      "  num_env_steps_sampled: 108000\n",
      "  num_env_steps_trained: 108000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-10-43\n",
      "done: false\n",
      "episode_len_mean: 493.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 493.24\n",
      "episode_reward_min: 213.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 524\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.0005859375\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.45233643615758545\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.003864577838862967\n",
      "        policy_loss: -0.022077939459072646\n",
      "        total_loss: 3.4593485078218604\n",
      "        vf_explained_var: -0.03708054615605262\n",
      "        vf_loss: 3.4814241868513887\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 24645.5\n",
      "  num_agent_steps_sampled: 108000\n",
      "  num_agent_steps_trained: 108000\n",
      "  num_env_steps_sampled: 108000\n",
      "  num_env_steps_trained: 108000\n",
      "iterations_since_restore: 27\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 108000\n",
      "num_agent_steps_trained: 108000\n",
      "num_env_steps_sampled: 108000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 108000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 14.514285714285716\n",
      "  ram_util_percent: 22.342857142857145\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08403675571067878\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04401751582723065\n",
      "  mean_inference_ms: 0.7455401153426906\n",
      "  mean_raw_obs_processing_ms: 0.20460180099149447\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004178047180175781\n",
      "    StateBufferConnector_ms: 0.003106832504272461\n",
      "    ViewRequirementAgentConnector_ms: 0.08489322662353516\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 493.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 493.24\n",
      "  episode_reward_min: 213.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      265, 500, 500, 500, 500, 213, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 471, 500, 500, 375, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 265.0, 500.0, 500.0, 500.0, 500.0, 213.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 471.0, 500.0, 500.0, 375.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08403675571067878\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04401751582723065\n",
      "    mean_inference_ms: 0.7455401153426906\n",
      "    mean_raw_obs_processing_ms: 0.20460180099149447\n",
      "time_since_restore: 272.34176111221313\n",
      "time_this_iter_s: 10.00688648223877\n",
      "time_total_s: 272.34176111221313\n",
      "timers:\n",
      "  learn_throughput: 695.422\n",
      "  learn_time_ms: 5751.907\n",
      "  load_throughput: 3565523.866\n",
      "  load_time_ms: 1.122\n",
      "  sample_time_ms: 4372.666\n",
      "  synch_weights_time_ms: 1.513\n",
      "  training_iteration_time_ms: 10127.812\n",
      "timestamp: 1683065443\n",
      "timesteps_total: 108000\n",
      "training_iteration: 27\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 112000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.004230499267578125\n",
      "  StateBufferConnector_ms: 0.0031092166900634766\n",
      "  ViewRequirementAgentConnector_ms: 0.08495926856994629\n",
      "counters:\n",
      "  num_agent_steps_sampled: 112000\n",
      "  num_agent_steps_trained: 112000\n",
      "  num_env_steps_sampled: 112000\n",
      "  num_env_steps_trained: 112000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-10-53\n",
      "done: false\n",
      "episode_len_mean: 493.24\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 493.24\n",
      "episode_reward_min: 213.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 532\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.00029296875\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.4392434811720284\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008698029346305812\n",
      "        policy_loss: -0.021882249432946404\n",
      "        total_loss: 0.046264090171203975\n",
      "        vf_explained_var: -0.10168515059255785\n",
      "        vf_loss: 0.06814379242363997\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 25575.5\n",
      "  num_agent_steps_sampled: 112000\n",
      "  num_agent_steps_trained: 112000\n",
      "  num_env_steps_sampled: 112000\n",
      "  num_env_steps_trained: 112000\n",
      "iterations_since_restore: 28\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 112000\n",
      "num_agent_steps_trained: 112000\n",
      "num_env_steps_sampled: 112000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 112000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 23.880000000000003\n",
      "  ram_util_percent: 22.399999999999995\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08405889504637112\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.044054968888979\n",
      "  mean_inference_ms: 0.7459349392548624\n",
      "  mean_raw_obs_processing_ms: 0.20458937117404166\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004230499267578125\n",
      "    StateBufferConnector_ms: 0.0031092166900634766\n",
      "    ViewRequirementAgentConnector_ms: 0.08495926856994629\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 493.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 493.24\n",
      "  episode_reward_min: 213.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 500, 500, 500, 265, 500, 500, 500, 500, 213, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      471, 500, 500, 375, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [500.0, 500.0, 500.0, 500.0, 265.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      213.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 471.0, 500.0, 500.0,\n",
      "      375.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08405889504637112\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.044054968888979\n",
      "    mean_inference_ms: 0.7459349392548624\n",
      "    mean_raw_obs_processing_ms: 0.20458937117404166\n",
      "time_since_restore: 282.67806363105774\n",
      "time_this_iter_s: 10.336302518844604\n",
      "time_total_s: 282.67806363105774\n",
      "timers:\n",
      "  learn_throughput: 693.628\n",
      "  learn_time_ms: 5766.778\n",
      "  load_throughput: 3575020.989\n",
      "  load_time_ms: 1.119\n",
      "  sample_time_ms: 4385.919\n",
      "  synch_weights_time_ms: 1.54\n",
      "  training_iteration_time_ms: 10155.964\n",
      "timestamp: 1683065453\n",
      "timesteps_total: 112000\n",
      "training_iteration: 28\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 116000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0040700435638427734\n",
      "  StateBufferConnector_ms: 0.0030755996704101562\n",
      "  ViewRequirementAgentConnector_ms: 0.0856325626373291\n",
      "counters:\n",
      "  num_agent_steps_sampled: 116000\n",
      "  num_agent_steps_trained: 116000\n",
      "  num_env_steps_sampled: 116000\n",
      "  num_env_steps_trained: 116000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-11-03\n",
      "done: false\n",
      "episode_len_mean: 495.59\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 495.59\n",
      "episode_reward_min: 213.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 540\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.00029296875\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.4913087706412039\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004059726293849602\n",
      "        policy_loss: -0.02180778047730846\n",
      "        total_loss: -0.01890928897210547\n",
      "        vf_explained_var: -0.16948134815821084\n",
      "        vf_loss: 0.0028973023031158523\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 26505.5\n",
      "  num_agent_steps_sampled: 116000\n",
      "  num_agent_steps_trained: 116000\n",
      "  num_env_steps_sampled: 116000\n",
      "  num_env_steps_trained: 116000\n",
      "iterations_since_restore: 29\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 116000\n",
      "num_agent_steps_trained: 116000\n",
      "num_env_steps_sampled: 116000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 116000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 23.392857142857142\n",
      "  ram_util_percent: 22.4\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08408072588820105\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.044094237358816575\n",
      "  mean_inference_ms: 0.7463484678132335\n",
      "  mean_raw_obs_processing_ms: 0.20458939593440967\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0040700435638427734\n",
      "    StateBufferConnector_ms: 0.0030755996704101562\n",
      "    ViewRequirementAgentConnector_ms: 0.0856325626373291\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 495.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 495.59\n",
      "  episode_reward_min: 213.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 213, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 471, 500, 500, 375, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [500.0, 213.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      471.0, 500.0, 500.0, 375.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08408072588820105\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.044094237358816575\n",
      "    mean_inference_ms: 0.7463484678132335\n",
      "    mean_raw_obs_processing_ms: 0.20458939593440967\n",
      "time_since_restore: 292.98650431632996\n",
      "time_this_iter_s: 10.308440685272217\n",
      "time_total_s: 292.98650431632996\n",
      "timers:\n",
      "  learn_throughput: 694.558\n",
      "  learn_time_ms: 5759.06\n",
      "  load_throughput: 3554269.008\n",
      "  load_time_ms: 1.125\n",
      "  sample_time_ms: 4396.38\n",
      "  synch_weights_time_ms: 1.516\n",
      "  training_iteration_time_ms: 10158.677\n",
      "timestamp: 1683065463\n",
      "timesteps_total: 116000\n",
      "training_iteration: 29\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 120000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.003999471664428711\n",
      "  StateBufferConnector_ms: 0.003068685531616211\n",
      "  ViewRequirementAgentConnector_ms: 0.08516955375671387\n",
      "counters:\n",
      "  num_agent_steps_sampled: 120000\n",
      "  num_agent_steps_trained: 120000\n",
      "  num_env_steps_sampled: 120000\n",
      "  num_env_steps_trained: 120000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-11-13\n",
      "done: false\n",
      "episode_len_mean: 497.73\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 497.73\n",
      "episode_reward_min: 375.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 548\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.000146484375\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5079514046189606\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009011219686107692\n",
      "        policy_loss: -0.0052605617911584916\n",
      "        total_loss: 0.8799725041274101\n",
      "        vf_explained_var: -0.2785147245853178\n",
      "        vf_loss: 0.8852317514497998\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 27435.5\n",
      "  num_agent_steps_sampled: 120000\n",
      "  num_agent_steps_trained: 120000\n",
      "  num_env_steps_sampled: 120000\n",
      "  num_env_steps_trained: 120000\n",
      "iterations_since_restore: 30\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 120000\n",
      "num_agent_steps_trained: 120000\n",
      "num_env_steps_sampled: 120000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 120000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 25.986666666666665\n",
      "  ram_util_percent: 22.7\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08409503816711289\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04412939558733493\n",
      "  mean_inference_ms: 0.7467136387063704\n",
      "  mean_raw_obs_processing_ms: 0.20458491640214188\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.003999471664428711\n",
      "    StateBufferConnector_ms: 0.003068685531616211\n",
      "    ViewRequirementAgentConnector_ms: 0.08516955375671387\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 497.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.73\n",
      "  episode_reward_min: 375.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      471, 500, 500, 375, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      427, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 471.0, 500.0, 500.0, 375.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 427.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08409503816711289\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04412939558733493\n",
      "    mean_inference_ms: 0.7467136387063704\n",
      "    mean_raw_obs_processing_ms: 0.20458491640214188\n",
      "time_since_restore: 303.2674195766449\n",
      "time_this_iter_s: 10.280915260314941\n",
      "time_total_s: 303.2674195766449\n",
      "timers:\n",
      "  learn_throughput: 692.896\n",
      "  learn_time_ms: 5772.869\n",
      "  load_throughput: 3415139.844\n",
      "  load_time_ms: 1.171\n",
      "  sample_time_ms: 4363.362\n",
      "  synch_weights_time_ms: 1.493\n",
      "  training_iteration_time_ms: 10139.5\n",
      "timestamp: 1683065473\n",
      "timesteps_total: 120000\n",
      "training_iteration: 30\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 124000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0045011043548583984\n",
      "  StateBufferConnector_ms: 0.003109455108642578\n",
      "  ViewRequirementAgentConnector_ms: 0.0863804817199707\n",
      "counters:\n",
      "  num_agent_steps_sampled: 124000\n",
      "  num_agent_steps_trained: 124000\n",
      "  num_env_steps_sampled: 124000\n",
      "  num_env_steps_trained: 124000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-11-24\n",
      "done: false\n",
      "episode_len_mean: 497.73\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 497.73\n",
      "episode_reward_min: 375.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 556\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.000146484375\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5132458541021552\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0030068619406091223\n",
      "        policy_loss: -0.01714937164878813\n",
      "        total_loss: -0.017113692656202176\n",
      "        vf_explained_var: -0.7467335393992803\n",
      "        vf_loss: 3.523831961570312e-05\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 28365.5\n",
      "  num_agent_steps_sampled: 124000\n",
      "  num_agent_steps_trained: 124000\n",
      "  num_env_steps_sampled: 124000\n",
      "  num_env_steps_trained: 124000\n",
      "iterations_since_restore: 31\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 124000\n",
      "num_agent_steps_trained: 124000\n",
      "num_env_steps_sampled: 124000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 124000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 12.893333333333333\n",
      "  ram_util_percent: 23.553333333333335\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08412658703861149\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.0441847414515998\n",
      "  mean_inference_ms: 0.747285042292273\n",
      "  mean_raw_obs_processing_ms: 0.20463719394651295\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0045011043548583984\n",
      "    StateBufferConnector_ms: 0.003109455108642578\n",
      "    ViewRequirementAgentConnector_ms: 0.0863804817199707\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 497.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 497.73\n",
      "  episode_reward_min: 375.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 500, 500, 500, 471, 500, 500, 375, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 427, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [500.0, 500.0, 500.0, 500.0, 471.0, 500.0, 500.0, 375.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 427.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08412658703861149\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0441847414515998\n",
      "    mean_inference_ms: 0.747285042292273\n",
      "    mean_raw_obs_processing_ms: 0.20463719394651295\n",
      "time_since_restore: 314.063595533371\n",
      "time_this_iter_s: 10.796175956726074\n",
      "time_total_s: 314.063595533371\n",
      "timers:\n",
      "  learn_throughput: 690.541\n",
      "  learn_time_ms: 5792.563\n",
      "  load_throughput: 3419803.91\n",
      "  load_time_ms: 1.17\n",
      "  sample_time_ms: 4421.562\n",
      "  synch_weights_time_ms: 1.49\n",
      "  training_iteration_time_ms: 10217.38\n",
      "timestamp: 1683065484\n",
      "timesteps_total: 124000\n",
      "training_iteration: 31\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 128000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.004534244537353516\n",
      "  StateBufferConnector_ms: 0.003094911575317383\n",
      "  ViewRequirementAgentConnector_ms: 0.08554363250732422\n",
      "counters:\n",
      "  num_agent_steps_sampled: 128000\n",
      "  num_agent_steps_trained: 128000\n",
      "  num_env_steps_sampled: 128000\n",
      "  num_env_steps_trained: 128000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-11-35\n",
      "done: false\n",
      "episode_len_mean: 488.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 488.9\n",
      "episode_reward_min: 121.0\n",
      "episodes_this_iter: 10\n",
      "episodes_total: 566\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 7.32421875e-05\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.48314993352659286\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006692823242204615\n",
      "        policy_loss: -0.0036195239873342616\n",
      "        total_loss: 1.1634189947318005\n",
      "        vf_explained_var: -0.7084794043853718\n",
      "        vf_loss: 1.167038009562234\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 29295.5\n",
      "  num_agent_steps_sampled: 128000\n",
      "  num_agent_steps_trained: 128000\n",
      "  num_env_steps_sampled: 128000\n",
      "  num_env_steps_trained: 128000\n",
      "iterations_since_restore: 32\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 128000\n",
      "num_agent_steps_trained: 128000\n",
      "num_env_steps_sampled: 128000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 128000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 21.08\n",
      "  ram_util_percent: 23.373333333333335\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08414864168747282\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.0442394208526911\n",
      "  mean_inference_ms: 0.7478133791292734\n",
      "  mean_raw_obs_processing_ms: 0.20466924903631323\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004534244537353516\n",
      "    StateBufferConnector_ms: 0.003094911575317383\n",
      "    ViewRequirementAgentConnector_ms: 0.08554363250732422\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 488.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 488.9\n",
      "  episode_reward_min: 121.0\n",
      "  episodes_this_iter: 10\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 427, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 214, 500, 500, 128, 500, 500, 121]\n",
      "    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 427.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 214.0, 500.0, 500.0, 128.0,\n",
      "      500.0, 500.0, 121.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08414864168747282\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0442394208526911\n",
      "    mean_inference_ms: 0.7478133791292734\n",
      "    mean_raw_obs_processing_ms: 0.20466924903631323\n",
      "time_since_restore: 324.2872452735901\n",
      "time_this_iter_s: 10.223649740219116\n",
      "time_total_s: 324.2872452735901\n",
      "timers:\n",
      "  learn_throughput: 690.019\n",
      "  learn_time_ms: 5796.946\n",
      "  load_throughput: 3446641.33\n",
      "  load_time_ms: 1.161\n",
      "  sample_time_ms: 4430.368\n",
      "  synch_weights_time_ms: 1.501\n",
      "  training_iteration_time_ms: 10230.57\n",
      "timestamp: 1683065495\n",
      "timesteps_total: 128000\n",
      "training_iteration: 32\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 132000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.004538536071777344\n",
      "  StateBufferConnector_ms: 0.003076314926147461\n",
      "  ViewRequirementAgentConnector_ms: 0.08523941040039062\n",
      "counters:\n",
      "  num_agent_steps_sampled: 132000\n",
      "  num_agent_steps_trained: 132000\n",
      "  num_env_steps_sampled: 132000\n",
      "  num_env_steps_trained: 132000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-11-45\n",
      "done: false\n",
      "episode_len_mean: 488.25\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 488.25\n",
      "episode_reward_min: 121.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 574\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 7.32421875e-05\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.4790510873640737\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0055318528493022556\n",
      "        policy_loss: -0.0038198003845830116\n",
      "        total_loss: 0.96796841010932\n",
      "        vf_explained_var: -0.7569518493067834\n",
      "        vf_loss: 0.9717878052104072\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 30225.5\n",
      "  num_agent_steps_sampled: 132000\n",
      "  num_agent_steps_trained: 132000\n",
      "  num_env_steps_sampled: 132000\n",
      "  num_env_steps_trained: 132000\n",
      "iterations_since_restore: 33\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 132000\n",
      "num_agent_steps_trained: 132000\n",
      "num_env_steps_sampled: 132000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 132000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 18.506666666666664\n",
      "  ram_util_percent: 23.320000000000004\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08416413330463829\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.044278677720663066\n",
      "  mean_inference_ms: 0.7482036010039116\n",
      "  mean_raw_obs_processing_ms: 0.20469240194148688\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004538536071777344\n",
      "    StateBufferConnector_ms: 0.003076314926147461\n",
      "    ViewRequirementAgentConnector_ms: 0.08523941040039062\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 488.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 488.25\n",
      "  episode_reward_min: 121.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 427, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 214, 500, 500, 128, 500, 500, 121,\n",
      "      500, 500, 500, 500, 500, 500, 435, 500]\n",
      "    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 427.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 214.0,\n",
      "      500.0, 500.0, 128.0, 500.0, 500.0, 121.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 435.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08416413330463829\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.044278677720663066\n",
      "    mean_inference_ms: 0.7482036010039116\n",
      "    mean_raw_obs_processing_ms: 0.20469240194148688\n",
      "time_since_restore: 334.46623182296753\n",
      "time_this_iter_s: 10.178986549377441\n",
      "time_total_s: 334.46623182296753\n",
      "timers:\n",
      "  learn_throughput: 690.115\n",
      "  learn_time_ms: 5796.138\n",
      "  load_throughput: 3258154.701\n",
      "  load_time_ms: 1.228\n",
      "  sample_time_ms: 4435.077\n",
      "  synch_weights_time_ms: 1.507\n",
      "  training_iteration_time_ms: 10234.55\n",
      "timestamp: 1683065505\n",
      "timesteps_total: 132000\n",
      "training_iteration: 33\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 136000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.004274129867553711\n",
      "  StateBufferConnector_ms: 0.0030345916748046875\n",
      "  ViewRequirementAgentConnector_ms: 0.08531975746154785\n",
      "counters:\n",
      "  num_agent_steps_sampled: 136000\n",
      "  num_agent_steps_trained: 136000\n",
      "  num_env_steps_sampled: 136000\n",
      "  num_env_steps_trained: 136000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-11-55\n",
      "done: false\n",
      "episode_len_mean: 488.25\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 488.25\n",
      "episode_reward_min: 121.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 582\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 7.32421875e-05\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.47250609715138714\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007394161635096897\n",
      "        policy_loss: -0.02172895861008475\n",
      "        total_loss: -0.021562105964028067\n",
      "        vf_explained_var: -0.2794299115416824\n",
      "        vf_loss: 0.000166312407683959\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 31155.5\n",
      "  num_agent_steps_sampled: 136000\n",
      "  num_agent_steps_trained: 136000\n",
      "  num_env_steps_sampled: 136000\n",
      "  num_env_steps_trained: 136000\n",
      "iterations_since_restore: 34\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 136000\n",
      "num_agent_steps_trained: 136000\n",
      "num_env_steps_sampled: 136000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 136000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 17.457142857142856\n",
      "  ram_util_percent: 23.321428571428577\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08418354496075307\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.044317040043585526\n",
      "  mean_inference_ms: 0.7485971074163765\n",
      "  mean_raw_obs_processing_ms: 0.2047209497464558\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004274129867553711\n",
      "    StateBufferConnector_ms: 0.0030345916748046875\n",
      "    ViewRequirementAgentConnector_ms: 0.08531975746154785\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 488.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 488.25\n",
      "  episode_reward_min: 121.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 427, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 214, 500, 500, 128, 500, 500, 121, 500, 500, 500, 500, 500, 500, 435, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 427.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 214.0, 500.0, 500.0, 128.0, 500.0, 500.0, 121.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 435.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08418354496075307\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.044317040043585526\n",
      "    mean_inference_ms: 0.7485971074163765\n",
      "    mean_raw_obs_processing_ms: 0.2047209497464558\n",
      "time_since_restore: 344.5671308040619\n",
      "time_this_iter_s: 10.10089898109436\n",
      "time_total_s: 344.5671308040619\n",
      "timers:\n",
      "  learn_throughput: 691.988\n",
      "  learn_time_ms: 5780.45\n",
      "  load_throughput: 3252659.17\n",
      "  load_time_ms: 1.23\n",
      "  sample_time_ms: 4436.11\n",
      "  synch_weights_time_ms: 1.539\n",
      "  training_iteration_time_ms: 10219.928\n",
      "timestamp: 1683065515\n",
      "timesteps_total: 136000\n",
      "training_iteration: 34\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 140000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0041522979736328125\n",
      "  StateBufferConnector_ms: 0.0030164718627929688\n",
      "  ViewRequirementAgentConnector_ms: 0.08527016639709473\n",
      "counters:\n",
      "  num_agent_steps_sampled: 140000\n",
      "  num_agent_steps_trained: 140000\n",
      "  num_env_steps_sampled: 140000\n",
      "  num_env_steps_trained: 140000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-12-05\n",
      "done: false\n",
      "episode_len_mean: 488.25\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 488.25\n",
      "episode_reward_min: 121.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 590\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 7.32421875e-05\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.4788990360113882\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013605709878126161\n",
      "        policy_loss: -0.014436831805474495\n",
      "        total_loss: -0.014419416449863904\n",
      "        vf_explained_var: -0.5884409374447279\n",
      "        vf_loss: 1.6418997596389012e-05\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 32085.5\n",
      "  num_agent_steps_sampled: 140000\n",
      "  num_agent_steps_trained: 140000\n",
      "  num_env_steps_sampled: 140000\n",
      "  num_env_steps_trained: 140000\n",
      "iterations_since_restore: 35\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 140000\n",
      "num_agent_steps_trained: 140000\n",
      "num_env_steps_sampled: 140000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 140000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 29.406666666666663\n",
      "  ram_util_percent: 23.373333333333335\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08420647773538076\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04435698284378166\n",
      "  mean_inference_ms: 0.7490063680212\n",
      "  mean_raw_obs_processing_ms: 0.20475759429657509\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0041522979736328125\n",
      "    StateBufferConnector_ms: 0.0030164718627929688\n",
      "    ViewRequirementAgentConnector_ms: 0.08527016639709473\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 488.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 488.25\n",
      "  episode_reward_min: 121.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 427, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 214, 500, 500, 128, 500, 500, 121,\n",
      "      500, 500, 500, 500, 500, 500, 435, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 427.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 214.0, 500.0, 500.0, 128.0, 500.0, 500.0,\n",
      "      121.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 435.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08420647773538076\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04435698284378166\n",
      "    mean_inference_ms: 0.7490063680212\n",
      "    mean_raw_obs_processing_ms: 0.20475759429657509\n",
      "time_since_restore: 355.0533196926117\n",
      "time_this_iter_s: 10.486188888549805\n",
      "time_total_s: 355.0533196926117\n",
      "timers:\n",
      "  learn_throughput: 687.882\n",
      "  learn_time_ms: 5814.952\n",
      "  load_throughput: 3269966.281\n",
      "  load_time_ms: 1.223\n",
      "  sample_time_ms: 4454.471\n",
      "  synch_weights_time_ms: 1.5\n",
      "  training_iteration_time_ms: 10272.749\n",
      "timestamp: 1683065525\n",
      "timesteps_total: 140000\n",
      "training_iteration: 35\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 144000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0042498111724853516\n",
      "  StateBufferConnector_ms: 0.0030312538146972656\n",
      "  ViewRequirementAgentConnector_ms: 0.08615946769714355\n",
      "counters:\n",
      "  num_agent_steps_sampled: 144000\n",
      "  num_agent_steps_trained: 144000\n",
      "  num_env_steps_sampled: 144000\n",
      "  num_env_steps_trained: 144000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-12-16\n",
      "done: false\n",
      "episode_len_mean: 488.25\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 488.25\n",
      "episode_reward_min: 121.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 598\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 7.32421875e-05\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.445405410214137\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005290582690839495\n",
      "        policy_loss: -0.005614593741233631\n",
      "        total_loss: -0.005608811267521433\n",
      "        vf_explained_var: -0.8778130559511083\n",
      "        vf_loss: 5.393605300024094e-06\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 33015.5\n",
      "  num_agent_steps_sampled: 144000\n",
      "  num_agent_steps_trained: 144000\n",
      "  num_env_steps_sampled: 144000\n",
      "  num_env_steps_trained: 144000\n",
      "iterations_since_restore: 36\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 144000\n",
      "num_agent_steps_trained: 144000\n",
      "num_env_steps_sampled: 144000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 144000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 24.439999999999998\n",
      "  ram_util_percent: 23.986666666666665\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08424154579642877\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04440703644722383\n",
      "  mean_inference_ms: 0.749513161637378\n",
      "  mean_raw_obs_processing_ms: 0.2048222002324809\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0042498111724853516\n",
      "    StateBufferConnector_ms: 0.0030312538146972656\n",
      "    ViewRequirementAgentConnector_ms: 0.08615946769714355\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 488.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 488.25\n",
      "  episode_reward_min: 121.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 427, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 214, 500, 500, 128, 500, 500, 121, 500, 500, 500, 500, 500, 500, 435, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      427.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 214.0, 500.0, 500.0,\n",
      "      128.0, 500.0, 500.0, 121.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 435.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08424154579642877\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04440703644722383\n",
      "    mean_inference_ms: 0.749513161637378\n",
      "    mean_raw_obs_processing_ms: 0.2048222002324809\n",
      "time_since_restore: 365.50358414649963\n",
      "time_this_iter_s: 10.45026445388794\n",
      "time_total_s: 365.50358414649963\n",
      "timers:\n",
      "  learn_throughput: 686.797\n",
      "  learn_time_ms: 5824.136\n",
      "  load_throughput: 3301756.637\n",
      "  load_time_ms: 1.211\n",
      "  sample_time_ms: 4486.582\n",
      "  synch_weights_time_ms: 1.486\n",
      "  training_iteration_time_ms: 10314.015\n",
      "timestamp: 1683065536\n",
      "timesteps_total: 144000\n",
      "training_iteration: 36\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 148000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.004258155822753906\n",
      "  StateBufferConnector_ms: 0.0030634403228759766\n",
      "  ViewRequirementAgentConnector_ms: 0.08650374412536621\n",
      "counters:\n",
      "  num_agent_steps_sampled: 148000\n",
      "  num_agent_steps_trained: 148000\n",
      "  num_env_steps_sampled: 148000\n",
      "  num_env_steps_trained: 148000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-12-26\n",
      "done: false\n",
      "episode_len_mean: 488.25\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 488.25\n",
      "episode_reward_min: 121.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 606\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 7.32421875e-05\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.4326988435560657\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.003619847410319519\n",
      "        policy_loss: -0.0011334699147971727\n",
      "        total_loss: -0.001130812836971174\n",
      "        vf_explained_var: -0.994742307611691\n",
      "        vf_loss: 2.3893523492554005e-06\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 33945.5\n",
      "  num_agent_steps_sampled: 148000\n",
      "  num_agent_steps_trained: 148000\n",
      "  num_env_steps_sampled: 148000\n",
      "  num_env_steps_trained: 148000\n",
      "iterations_since_restore: 37\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 148000\n",
      "num_agent_steps_trained: 148000\n",
      "num_env_steps_sampled: 148000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 148000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 24.271428571428572\n",
      "  ram_util_percent: 23.757142857142856\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08428026850225427\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04445748343626635\n",
      "  mean_inference_ms: 0.7500369815190493\n",
      "  mean_raw_obs_processing_ms: 0.2048975642681357\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004258155822753906\n",
      "    StateBufferConnector_ms: 0.0030634403228759766\n",
      "    ViewRequirementAgentConnector_ms: 0.08650374412536621\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 488.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 488.25\n",
      "  episode_reward_min: 121.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 427, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 214, 500, 500, 128, 500, 500, 121,\n",
      "      500, 500, 500, 500, 500, 500, 435, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 427.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      214.0, 500.0, 500.0, 128.0, 500.0, 500.0, 121.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 435.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08428026850225427\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04445748343626635\n",
      "    mean_inference_ms: 0.7500369815190493\n",
      "    mean_raw_obs_processing_ms: 0.2048975642681357\n",
      "time_since_restore: 375.5377399921417\n",
      "time_this_iter_s: 10.03415584564209\n",
      "time_total_s: 375.5377399921417\n",
      "timers:\n",
      "  learn_throughput: 686.846\n",
      "  learn_time_ms: 5823.723\n",
      "  load_throughput: 3310617.44\n",
      "  load_time_ms: 1.208\n",
      "  sample_time_ms: 4489.758\n",
      "  synch_weights_time_ms: 1.482\n",
      "  training_iteration_time_ms: 10316.767\n",
      "timestamp: 1683065546\n",
      "timesteps_total: 148000\n",
      "training_iteration: 37\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 152000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.00428462028503418\n",
      "  StateBufferConnector_ms: 0.0030536651611328125\n",
      "  ViewRequirementAgentConnector_ms: 0.08647847175598145\n",
      "counters:\n",
      "  num_agent_steps_sampled: 152000\n",
      "  num_agent_steps_trained: 152000\n",
      "  num_env_steps_sampled: 152000\n",
      "  num_env_steps_trained: 152000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-12-36\n",
      "done: false\n",
      "episode_len_mean: 488.25\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 488.25\n",
      "episode_reward_min: 121.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 614\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.662109375e-05\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.42385270531459524\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007685923844787828\n",
      "        policy_loss: -0.004032364375488732\n",
      "        total_loss: -0.0040278595381526535\n",
      "        vf_explained_var: -1.0\n",
      "        vf_loss: 4.2217354416851614e-06\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 34875.5\n",
      "  num_agent_steps_sampled: 152000\n",
      "  num_agent_steps_trained: 152000\n",
      "  num_env_steps_sampled: 152000\n",
      "  num_env_steps_trained: 152000\n",
      "iterations_since_restore: 38\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 152000\n",
      "num_agent_steps_trained: 152000\n",
      "num_env_steps_sampled: 152000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 152000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 20.42142857142857\n",
      "  ram_util_percent: 23.600000000000005\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08432176762282559\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.044507754326690366\n",
      "  mean_inference_ms: 0.7505812587429911\n",
      "  mean_raw_obs_processing_ms: 0.20497990353491816\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.00428462028503418\n",
      "    StateBufferConnector_ms: 0.0030536651611328125\n",
      "    ViewRequirementAgentConnector_ms: 0.08647847175598145\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 488.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 488.25\n",
      "  episode_reward_min: 121.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 427, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 214, 500, 500, 128, 500, 500, 121, 500, 500, 500, 500, 500, 500, 435, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 427.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 214.0, 500.0, 500.0, 128.0, 500.0, 500.0, 121.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 435.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08432176762282559\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.044507754326690366\n",
      "    mean_inference_ms: 0.7505812587429911\n",
      "    mean_raw_obs_processing_ms: 0.20497990353491816\n",
      "time_since_restore: 385.63077688217163\n",
      "time_this_iter_s: 10.093036890029907\n",
      "time_total_s: 385.63077688217163\n",
      "timers:\n",
      "  learn_throughput: 688.567\n",
      "  learn_time_ms: 5809.166\n",
      "  load_throughput: 3294106.929\n",
      "  load_time_ms: 1.214\n",
      "  sample_time_ms: 4480.057\n",
      "  synch_weights_time_ms: 1.462\n",
      "  training_iteration_time_ms: 10292.483\n",
      "timestamp: 1683065556\n",
      "timesteps_total: 152000\n",
      "training_iteration: 38\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 156000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.004286050796508789\n",
      "  StateBufferConnector_ms: 0.003072023391723633\n",
      "  ViewRequirementAgentConnector_ms: 0.08634829521179199\n",
      "counters:\n",
      "  num_agent_steps_sampled: 156000\n",
      "  num_agent_steps_trained: 156000\n",
      "  num_env_steps_sampled: 156000\n",
      "  num_env_steps_trained: 156000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-12-46\n",
      "done: false\n",
      "episode_len_mean: 488.25\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 488.25\n",
      "episode_reward_min: 121.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 622\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.662109375e-05\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.4171483040817322\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007797694857375596\n",
      "        policy_loss: 0.000716354024987067\n",
      "        total_loss: 0.0007177781273600876\n",
      "        vf_explained_var: -0.8649410251648195\n",
      "        vf_loss: 1.1394301328505453e-06\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 35805.5\n",
      "  num_agent_steps_sampled: 156000\n",
      "  num_agent_steps_trained: 156000\n",
      "  num_env_steps_sampled: 156000\n",
      "  num_env_steps_trained: 156000\n",
      "iterations_since_restore: 39\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 156000\n",
      "num_agent_steps_trained: 156000\n",
      "num_env_steps_sampled: 156000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 156000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 21.713333333333335\n",
      "  ram_util_percent: 23.60000000000001\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0843633912306241\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04455708065574532\n",
      "  mean_inference_ms: 0.7511140838693816\n",
      "  mean_raw_obs_processing_ms: 0.2050616506757985\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004286050796508789\n",
      "    StateBufferConnector_ms: 0.003072023391723633\n",
      "    ViewRequirementAgentConnector_ms: 0.08634829521179199\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 488.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 488.25\n",
      "  episode_reward_min: 121.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 427, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 214, 500, 500, 128, 500, 500, 121,\n",
      "      500, 500, 500, 500, 500, 500, 435, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 427.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 214.0, 500.0, 500.0, 128.0, 500.0,\n",
      "      500.0, 121.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 435.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0843633912306241\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04455708065574532\n",
      "    mean_inference_ms: 0.7511140838693816\n",
      "    mean_raw_obs_processing_ms: 0.2050616506757985\n",
      "time_since_restore: 395.6130301952362\n",
      "time_this_iter_s: 9.982253313064575\n",
      "time_total_s: 395.6130301952362\n",
      "timers:\n",
      "  learn_throughput: 690.264\n",
      "  learn_time_ms: 5794.882\n",
      "  load_throughput: 3260117.368\n",
      "  load_time_ms: 1.227\n",
      "  sample_time_ms: 4461.716\n",
      "  synch_weights_time_ms: 1.462\n",
      "  training_iteration_time_ms: 10259.869\n",
      "timestamp: 1683065566\n",
      "timesteps_total: 156000\n",
      "training_iteration: 39\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 160000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.004251241683959961\n",
      "  StateBufferConnector_ms: 0.003109455108642578\n",
      "  ViewRequirementAgentConnector_ms: 0.08634471893310547\n",
      "counters:\n",
      "  num_agent_steps_sampled: 160000\n",
      "  num_agent_steps_trained: 160000\n",
      "  num_env_steps_sampled: 160000\n",
      "  num_env_steps_trained: 160000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-12-56\n",
      "done: false\n",
      "episode_len_mean: 488.25\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 488.25\n",
      "episode_reward_min: 121.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 630\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.662109375e-05\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.47526960324856543\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019923286907653104\n",
      "        policy_loss: -0.002285159723733061\n",
      "        total_loss: -0.0022832101551435325\n",
      "        vf_explained_var: -0.7077816873468379\n",
      "        vf_loss: 1.2184269097258127e-06\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 36735.5\n",
      "  num_agent_steps_sampled: 160000\n",
      "  num_agent_steps_trained: 160000\n",
      "  num_env_steps_sampled: 160000\n",
      "  num_env_steps_trained: 160000\n",
      "iterations_since_restore: 40\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 160000\n",
      "num_agent_steps_trained: 160000\n",
      "num_env_steps_sampled: 160000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 160000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 17.778571428571432\n",
      "  ram_util_percent: 23.600000000000005\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08440078239698329\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04460034105062566\n",
      "  mean_inference_ms: 0.751593280868328\n",
      "  mean_raw_obs_processing_ms: 0.20513308876929634\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004251241683959961\n",
      "    StateBufferConnector_ms: 0.003109455108642578\n",
      "    ViewRequirementAgentConnector_ms: 0.08634471893310547\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 488.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 488.25\n",
      "  episode_reward_min: 121.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 427, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 214, 500, 500, 128, 500, 500, 121, 500, 500, 500, 500, 500, 500, 435, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 427.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 214.0, 500.0,\n",
      "      500.0, 128.0, 500.0, 500.0, 121.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      435.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08440078239698329\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04460034105062566\n",
      "    mean_inference_ms: 0.751593280868328\n",
      "    mean_raw_obs_processing_ms: 0.20513308876929634\n",
      "time_since_restore: 405.8116581439972\n",
      "time_this_iter_s: 10.198627948760986\n",
      "time_total_s: 405.8116581439972\n",
      "timers:\n",
      "  learn_throughput: 691.946\n",
      "  learn_time_ms: 5780.796\n",
      "  load_throughput: 3315327.734\n",
      "  load_time_ms: 1.207\n",
      "  sample_time_ms: 4467.604\n",
      "  synch_weights_time_ms: 1.485\n",
      "  training_iteration_time_ms: 10251.663\n",
      "timestamp: 1683065576\n",
      "timesteps_total: 160000\n",
      "training_iteration: 40\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 164000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.004296541213989258\n",
      "  StateBufferConnector_ms: 0.003137826919555664\n",
      "  ViewRequirementAgentConnector_ms: 0.08617806434631348\n",
      "counters:\n",
      "  num_agent_steps_sampled: 164000\n",
      "  num_agent_steps_trained: 164000\n",
      "  num_env_steps_sampled: 164000\n",
      "  num_env_steps_trained: 164000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-13-06\n",
      "done: false\n",
      "episode_len_mean: 484.83\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 484.83\n",
      "episode_reward_min: 121.0\n",
      "episodes_this_iter: 9\n",
      "episodes_total: 639\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.662109375e-05\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.4056061880562895\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009893561805613035\n",
      "        policy_loss: -0.004298892200634044\n",
      "        total_loss: 0.3938879798496923\n",
      "        vf_explained_var: -0.9042018371243631\n",
      "        vf_loss: 0.3981864949016091\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 37665.5\n",
      "  num_agent_steps_sampled: 164000\n",
      "  num_agent_steps_trained: 164000\n",
      "  num_env_steps_sampled: 164000\n",
      "  num_env_steps_trained: 164000\n",
      "iterations_since_restore: 41\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 164000\n",
      "num_agent_steps_trained: 164000\n",
      "num_env_steps_sampled: 164000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 164000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 21.393333333333334\n",
      "  ram_util_percent: 23.639999999999997\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08444062185368006\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.0446430491938776\n",
      "  mean_inference_ms: 0.7520764164103784\n",
      "  mean_raw_obs_processing_ms: 0.2052034682764178\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004296541213989258\n",
      "    StateBufferConnector_ms: 0.003137826919555664\n",
      "    ViewRequirementAgentConnector_ms: 0.08617806434631348\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 484.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 484.83\n",
      "  episode_reward_min: 121.0\n",
      "  episodes_this_iter: 9\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 427, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 214, 500, 500, 128, 500, 500, 121, 500,\n",
      "      500, 500, 500, 500, 500, 435, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 158, 500, 500, 500]\n",
      "    episode_reward: [500.0, 427.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      214.0, 500.0, 500.0, 128.0, 500.0, 500.0, 121.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 435.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 158.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08444062185368006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0446430491938776\n",
      "    mean_inference_ms: 0.7520764164103784\n",
      "    mean_raw_obs_processing_ms: 0.2052034682764178\n",
      "time_since_restore: 416.0375168323517\n",
      "time_this_iter_s: 10.225858688354492\n",
      "time_total_s: 416.0375168323517\n",
      "timers:\n",
      "  learn_throughput: 693.507\n",
      "  learn_time_ms: 5767.787\n",
      "  load_throughput: 3290682.567\n",
      "  load_time_ms: 1.216\n",
      "  sample_time_ms: 4423.529\n",
      "  synch_weights_time_ms: 1.488\n",
      "  training_iteration_time_ms: 10194.595\n",
      "timestamp: 1683065586\n",
      "timesteps_total: 164000\n",
      "training_iteration: 41\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 168000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.004320383071899414\n",
      "  StateBufferConnector_ms: 0.003154754638671875\n",
      "  ViewRequirementAgentConnector_ms: 0.08646893501281738\n",
      "counters:\n",
      "  num_agent_steps_sampled: 168000\n",
      "  num_agent_steps_trained: 168000\n",
      "  num_env_steps_sampled: 168000\n",
      "  num_env_steps_trained: 168000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-13-17\n",
      "done: false\n",
      "episode_len_mean: 485.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 485.56\n",
      "episode_reward_min: 121.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 647\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.662109375e-05\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.39432475624545926\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00497123062138437\n",
      "        policy_loss: -0.001451683949719193\n",
      "        total_loss: -0.0014504468689362207\n",
      "        vf_explained_var: -0.7636198916101968\n",
      "        vf_loss: 1.0544816373161451e-06\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 38595.5\n",
      "  num_agent_steps_sampled: 168000\n",
      "  num_agent_steps_trained: 168000\n",
      "  num_env_steps_sampled: 168000\n",
      "  num_env_steps_trained: 168000\n",
      "iterations_since_restore: 42\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 168000\n",
      "num_agent_steps_trained: 168000\n",
      "num_env_steps_sampled: 168000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 168000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 22.32857142857143\n",
      "  ram_util_percent: 23.628571428571433\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08448180612236009\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04468232248915431\n",
      "  mean_inference_ms: 0.7525320187216074\n",
      "  mean_raw_obs_processing_ms: 0.20527416475228505\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.004320383071899414\n",
      "    StateBufferConnector_ms: 0.003154754638671875\n",
      "    ViewRequirementAgentConnector_ms: 0.08646893501281738\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 485.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 485.56\n",
      "  episode_reward_min: 121.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      214, 500, 500, 128, 500, 500, 121, 500, 500, 500, 500, 500, 500, 435, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 158, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 214.0, 500.0, 500.0, 128.0, 500.0, 500.0, 121.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 435.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 158.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08448180612236009\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04468232248915431\n",
      "    mean_inference_ms: 0.7525320187216074\n",
      "    mean_raw_obs_processing_ms: 0.20527416475228505\n",
      "time_since_restore: 426.2843472957611\n",
      "time_this_iter_s: 10.246830463409424\n",
      "time_total_s: 426.2843472957611\n",
      "timers:\n",
      "  learn_throughput: 693.693\n",
      "  learn_time_ms: 5766.239\n",
      "  load_throughput: 3096055.657\n",
      "  load_time_ms: 1.292\n",
      "  sample_time_ms: 4427.293\n",
      "  synch_weights_time_ms: 1.501\n",
      "  training_iteration_time_ms: 10196.906\n",
      "timestamp: 1683065597\n",
      "timesteps_total: 168000\n",
      "training_iteration: 42\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 172000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.003847837448120117\n",
      "  StateBufferConnector_ms: 0.003103494644165039\n",
      "  ViewRequirementAgentConnector_ms: 0.08524703979492188\n",
      "counters:\n",
      "  num_agent_steps_sampled: 172000\n",
      "  num_agent_steps_trained: 172000\n",
      "  num_env_steps_sampled: 172000\n",
      "  num_env_steps_trained: 172000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-13-27\n",
      "done: false\n",
      "episode_len_mean: 485.56\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 485.56\n",
      "episode_reward_min: 121.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 655\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8310546875e-05\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.4093787298087151\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015621709234886481\n",
      "        policy_loss: -0.009410362754778195\n",
      "        total_loss: -0.009409654048341577\n",
      "        vf_explained_var: -0.8590672009734697\n",
      "        vf_loss: 4.227567309479577e-07\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 39525.5\n",
      "  num_agent_steps_sampled: 172000\n",
      "  num_agent_steps_trained: 172000\n",
      "  num_env_steps_sampled: 172000\n",
      "  num_env_steps_trained: 172000\n",
      "iterations_since_restore: 43\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 172000\n",
      "num_agent_steps_trained: 172000\n",
      "num_env_steps_sampled: 172000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 172000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 21.813333333333336\n",
      "  ram_util_percent: 23.59333333333334\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08450502579300148\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.044701279210263005\n",
      "  mean_inference_ms: 0.7527703125517567\n",
      "  mean_raw_obs_processing_ms: 0.20529473852595095\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.003847837448120117\n",
      "    StateBufferConnector_ms: 0.003103494644165039\n",
      "    ViewRequirementAgentConnector_ms: 0.08524703979492188\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 485.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 485.56\n",
      "  episode_reward_min: 121.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 500, 500, 500, 214, 500, 500, 128, 500, 500, 121, 500,\n",
      "      500, 500, 500, 500, 500, 435, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 158, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [500.0, 500.0, 500.0, 500.0, 214.0, 500.0, 500.0, 128.0, 500.0,\n",
      "      500.0, 121.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 435.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 158.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08450502579300148\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.044701279210263005\n",
      "    mean_inference_ms: 0.7527703125517567\n",
      "    mean_raw_obs_processing_ms: 0.20529473852595095\n",
      "time_since_restore: 436.38037157058716\n",
      "time_this_iter_s: 10.09602427482605\n",
      "time_total_s: 436.38037157058716\n",
      "timers:\n",
      "  learn_throughput: 694.226\n",
      "  learn_time_ms: 5761.815\n",
      "  load_throughput: 3291973.942\n",
      "  load_time_ms: 1.215\n",
      "  sample_time_ms: 4423.43\n",
      "  synch_weights_time_ms: 1.499\n",
      "  training_iteration_time_ms: 10188.536\n",
      "timestamp: 1683065607\n",
      "timesteps_total: 172000\n",
      "training_iteration: 43\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 176000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0037593841552734375\n",
      "  StateBufferConnector_ms: 0.0031278133392333984\n",
      "  ViewRequirementAgentConnector_ms: 0.08528852462768555\n",
      "counters:\n",
      "  num_agent_steps_sampled: 176000\n",
      "  num_agent_steps_trained: 176000\n",
      "  num_env_steps_sampled: 176000\n",
      "  num_env_steps_trained: 176000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-13-37\n",
      "done: false\n",
      "episode_len_mean: 488.02\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 488.02\n",
      "episode_reward_min: 121.0\n",
      "episodes_this_iter: 9\n",
      "episodes_total: 664\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8310546875e-05\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.4483386835744304\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007927638284691691\n",
      "        policy_loss: -0.04362658880731111\n",
      "        total_loss: 1.3103696237007776\n",
      "        vf_explained_var: -0.7903337201123597\n",
      "        vf_loss: 1.3539960848675687\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 40455.5\n",
      "  num_agent_steps_sampled: 176000\n",
      "  num_agent_steps_trained: 176000\n",
      "  num_env_steps_sampled: 176000\n",
      "  num_env_steps_trained: 176000\n",
      "iterations_since_restore: 44\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 176000\n",
      "num_agent_steps_trained: 176000\n",
      "num_env_steps_sampled: 176000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 176000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 22.507142857142856\n",
      "  ram_util_percent: 23.600000000000005\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08452450658750528\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04471456844476544\n",
      "  mean_inference_ms: 0.7529567254354785\n",
      "  mean_raw_obs_processing_ms: 0.20530262674793065\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0037593841552734375\n",
      "    StateBufferConnector_ms: 0.0031278133392333984\n",
      "    ViewRequirementAgentConnector_ms: 0.08528852462768555\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 488.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 488.02\n",
      "  episode_reward_min: 121.0\n",
      "  episodes_this_iter: 9\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 121, 500, 500, 500, 500, 500, 500, 435, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 158, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 400, 188]\n",
      "    episode_reward: [500.0, 121.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 435.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 158.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 400.0, 188.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08452450658750528\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04471456844476544\n",
      "    mean_inference_ms: 0.7529567254354785\n",
      "    mean_raw_obs_processing_ms: 0.20530262674793065\n",
      "time_since_restore: 446.39750361442566\n",
      "time_this_iter_s: 10.017132043838501\n",
      "time_total_s: 446.39750361442566\n",
      "timers:\n",
      "  learn_throughput: 694.771\n",
      "  learn_time_ms: 5757.292\n",
      "  load_throughput: 3312382.231\n",
      "  load_time_ms: 1.208\n",
      "  sample_time_ms: 4419.581\n",
      "  synch_weights_time_ms: 1.489\n",
      "  training_iteration_time_ms: 10180.147\n",
      "timestamp: 1683065617\n",
      "timesteps_total: 176000\n",
      "training_iteration: 44\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 180000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0037527084350585938\n",
      "  StateBufferConnector_ms: 0.0031702518463134766\n",
      "  ViewRequirementAgentConnector_ms: 0.08547067642211914\n",
      "counters:\n",
      "  num_agent_steps_sampled: 180000\n",
      "  num_agent_steps_trained: 180000\n",
      "  num_env_steps_sampled: 180000\n",
      "  num_env_steps_trained: 180000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-13-47\n",
      "done: false\n",
      "episode_len_mean: 491.81\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 491.81\n",
      "episode_reward_min: 158.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 672\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8310546875e-05\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.40710097355868224\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0034471940323184446\n",
      "        policy_loss: -0.013188545319742413\n",
      "        total_loss: -0.012920663457724356\n",
      "        vf_explained_var: -0.6507316115081951\n",
      "        vf_loss: 0.0002678189623522133\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 41385.5\n",
      "  num_agent_steps_sampled: 180000\n",
      "  num_agent_steps_trained: 180000\n",
      "  num_env_steps_sampled: 180000\n",
      "  num_env_steps_trained: 180000\n",
      "iterations_since_restore: 45\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 180000\n",
      "num_agent_steps_trained: 180000\n",
      "num_env_steps_sampled: 180000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 180000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 20.485714285714288\n",
      "  ram_util_percent: 23.600000000000005\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08454042680178789\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.044724045368763045\n",
      "  mean_inference_ms: 0.7530963771507211\n",
      "  mean_raw_obs_processing_ms: 0.20530731154783288\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0037527084350585938\n",
      "    StateBufferConnector_ms: 0.0031702518463134766\n",
      "    ViewRequirementAgentConnector_ms: 0.08547067642211914\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 491.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 491.81\n",
      "  episode_reward_min: 158.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [435, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 158, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 400, 188,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [435.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 158.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 400.0, 188.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08454042680178789\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.044724045368763045\n",
      "    mean_inference_ms: 0.7530963771507211\n",
      "    mean_raw_obs_processing_ms: 0.20530731154783288\n",
      "time_since_restore: 456.461124420166\n",
      "time_this_iter_s: 10.063620805740356\n",
      "time_total_s: 456.461124420166\n",
      "timers:\n",
      "  learn_throughput: 698.945\n",
      "  learn_time_ms: 5722.914\n",
      "  load_throughput: 3272453.772\n",
      "  load_time_ms: 1.222\n",
      "  sample_time_ms: 4411.69\n",
      "  synch_weights_time_ms: 1.486\n",
      "  training_iteration_time_ms: 10137.891\n",
      "timestamp: 1683065627\n",
      "timesteps_total: 180000\n",
      "training_iteration: 45\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 184000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0037250518798828125\n",
      "  StateBufferConnector_ms: 0.003163576126098633\n",
      "  ViewRequirementAgentConnector_ms: 0.08521199226379395\n",
      "counters:\n",
      "  num_agent_steps_sampled: 184000\n",
      "  num_agent_steps_trained: 184000\n",
      "  num_env_steps_sampled: 184000\n",
      "  num_env_steps_trained: 184000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-13-57\n",
      "done: false\n",
      "episode_len_mean: 492.46\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 492.46\n",
      "episode_reward_min: 158.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 680\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 9.1552734375e-06\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.3904971918912344\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00825424519456728\n",
      "        policy_loss: -0.010021941111453117\n",
      "        total_loss: -0.00995379557172137\n",
      "        vf_explained_var: -0.7860769203273199\n",
      "        vf_loss: 6.80692302369128e-05\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 42315.5\n",
      "  num_agent_steps_sampled: 184000\n",
      "  num_agent_steps_trained: 184000\n",
      "  num_env_steps_sampled: 184000\n",
      "  num_env_steps_trained: 184000\n",
      "iterations_since_restore: 46\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 184000\n",
      "num_agent_steps_trained: 184000\n",
      "num_env_steps_sampled: 184000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 184000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 22.206666666666667\n",
      "  ram_util_percent: 23.60000000000001\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0845539499685885\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04473179717080186\n",
      "  mean_inference_ms: 0.7532130472699572\n",
      "  mean_raw_obs_processing_ms: 0.205308041823764\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0037250518798828125\n",
      "    StateBufferConnector_ms: 0.003163576126098633\n",
      "    ViewRequirementAgentConnector_ms: 0.08521199226379395\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 492.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 492.46\n",
      "  episode_reward_min: 158.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 158, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 400, 188, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500]\n",
      "    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 158.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 400.0, 188.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0845539499685885\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04473179717080186\n",
      "    mean_inference_ms: 0.7532130472699572\n",
      "    mean_raw_obs_processing_ms: 0.205308041823764\n",
      "time_since_restore: 466.6509783267975\n",
      "time_this_iter_s: 10.18985390663147\n",
      "time_total_s: 466.6509783267975\n",
      "timers:\n",
      "  learn_throughput: 697.976\n",
      "  learn_time_ms: 5730.857\n",
      "  load_throughput: 3265956.005\n",
      "  load_time_ms: 1.225\n",
      "  sample_time_ms: 4377.662\n",
      "  synch_weights_time_ms: 1.509\n",
      "  training_iteration_time_ms: 10111.833\n",
      "timestamp: 1683065637\n",
      "timesteps_total: 184000\n",
      "training_iteration: 46\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 188000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.003756284713745117\n",
      "  StateBufferConnector_ms: 0.0031690597534179688\n",
      "  ViewRequirementAgentConnector_ms: 0.08527088165283203\n",
      "counters:\n",
      "  num_agent_steps_sampled: 188000\n",
      "  num_agent_steps_trained: 188000\n",
      "  num_env_steps_sampled: 188000\n",
      "  num_env_steps_trained: 188000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-14-07\n",
      "done: false\n",
      "episode_len_mean: 484.39\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 484.39\n",
      "episode_reward_min: 158.0\n",
      "episodes_this_iter: 9\n",
      "episodes_total: 689\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 9.1552734375e-06\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.4150503192697802\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005924102425675696\n",
      "        policy_loss: 0.003489550749861425\n",
      "        total_loss: 6.8952839683030325\n",
      "        vf_explained_var: -0.21143461331244437\n",
      "        vf_loss: 6.891794418703805\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 43245.5\n",
      "  num_agent_steps_sampled: 188000\n",
      "  num_agent_steps_trained: 188000\n",
      "  num_env_steps_sampled: 188000\n",
      "  num_env_steps_trained: 188000\n",
      "iterations_since_restore: 47\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 188000\n",
      "num_agent_steps_trained: 188000\n",
      "num_env_steps_sampled: 188000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 188000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 17.25714285714286\n",
      "  ram_util_percent: 23.600000000000005\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08456535637609214\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.044736263207502465\n",
      "  mean_inference_ms: 0.7532936568796216\n",
      "  mean_raw_obs_processing_ms: 0.20529945652923487\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.003756284713745117\n",
      "    StateBufferConnector_ms: 0.0031690597534179688\n",
      "    ViewRequirementAgentConnector_ms: 0.08527088165283203\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 484.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 484.39\n",
      "  episode_reward_min: 158.0\n",
      "  episodes_this_iter: 9\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 158, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 400, 188, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 418,\n",
      "      426, 445, 384, 500, 401, 427, 324, 368]\n",
      "    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 158.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 400.0, 188.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 418.0, 426.0, 445.0, 384.0, 500.0, 401.0,\n",
      "      427.0, 324.0, 368.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08456535637609214\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.044736263207502465\n",
      "    mean_inference_ms: 0.7532936568796216\n",
      "    mean_raw_obs_processing_ms: 0.20529945652923487\n",
      "time_since_restore: 476.66170501708984\n",
      "time_this_iter_s: 10.010726690292358\n",
      "time_total_s: 476.66170501708984\n",
      "timers:\n",
      "  learn_throughput: 697.83\n",
      "  learn_time_ms: 5732.059\n",
      "  load_throughput: 2929903.95\n",
      "  load_time_ms: 1.365\n",
      "  sample_time_ms: 4373.963\n",
      "  synch_weights_time_ms: 1.509\n",
      "  training_iteration_time_ms: 10109.482\n",
      "timestamp: 1683065647\n",
      "timesteps_total: 188000\n",
      "training_iteration: 47\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 192000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0036995410919189453\n",
      "  StateBufferConnector_ms: 0.003118753433227539\n",
      "  ViewRequirementAgentConnector_ms: 0.08373093605041504\n",
      "counters:\n",
      "  num_agent_steps_sampled: 192000\n",
      "  num_agent_steps_trained: 192000\n",
      "  num_env_steps_sampled: 192000\n",
      "  num_env_steps_trained: 192000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-14-17\n",
      "done: false\n",
      "episode_len_mean: 475.9\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 475.9\n",
      "episode_reward_min: 158.0\n",
      "episodes_this_iter: 10\n",
      "episodes_total: 699\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 9.1552734375e-06\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.394559763596263\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005206969415264061\n",
      "        policy_loss: 0.0023090384339773526\n",
      "        total_loss: 5.84619400529266\n",
      "        vf_explained_var: -0.331172590678738\n",
      "        vf_loss: 5.843884958767442\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 44175.5\n",
      "  num_agent_steps_sampled: 192000\n",
      "  num_agent_steps_trained: 192000\n",
      "  num_env_steps_sampled: 192000\n",
      "  num_env_steps_trained: 192000\n",
      "iterations_since_restore: 48\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 192000\n",
      "num_agent_steps_trained: 192000\n",
      "num_env_steps_sampled: 192000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 192000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 21.342857142857145\n",
      "  ram_util_percent: 23.600000000000005\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08456162785300007\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.0447241845492013\n",
      "  mean_inference_ms: 0.7532123434256771\n",
      "  mean_raw_obs_processing_ms: 0.2052497717031156\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0036995410919189453\n",
      "    StateBufferConnector_ms: 0.003118753433227539\n",
      "    ViewRequirementAgentConnector_ms: 0.08373093605041504\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 475.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 475.9\n",
      "  episode_reward_min: 158.0\n",
      "  episodes_this_iter: 10\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 158, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 400, 188, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 418, 426, 445, 384, 500, 401, 427, 324, 368, 500, 284,\n",
      "      444, 500, 500, 429, 430, 391, 318, 355]\n",
      "    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 158.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 400.0,\n",
      "      188.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 418.0, 426.0, 445.0, 384.0, 500.0,\n",
      "      401.0, 427.0, 324.0, 368.0, 500.0, 284.0, 444.0, 500.0, 500.0, 429.0, 430.0,\n",
      "      391.0, 318.0, 355.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08456162785300007\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0447241845492013\n",
      "    mean_inference_ms: 0.7532123434256771\n",
      "    mean_raw_obs_processing_ms: 0.2052497717031156\n",
      "time_since_restore: 486.5989921092987\n",
      "time_this_iter_s: 9.937287092208862\n",
      "time_total_s: 486.5989921092987\n",
      "timers:\n",
      "  learn_throughput: 698.097\n",
      "  learn_time_ms: 5729.859\n",
      "  load_throughput: 2879714.384\n",
      "  load_time_ms: 1.389\n",
      "  sample_time_ms: 4360.529\n",
      "  synch_weights_time_ms: 1.506\n",
      "  training_iteration_time_ms: 10093.872\n",
      "timestamp: 1683065657\n",
      "timesteps_total: 192000\n",
      "training_iteration: 48\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 196000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0037021636962890625\n",
      "  StateBufferConnector_ms: 0.0030972957611083984\n",
      "  ViewRequirementAgentConnector_ms: 0.08389115333557129\n",
      "counters:\n",
      "  num_agent_steps_sampled: 196000\n",
      "  num_agent_steps_trained: 196000\n",
      "  num_env_steps_sampled: 196000\n",
      "  num_env_steps_trained: 196000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-14-27\n",
      "done: false\n",
      "episode_len_mean: 471.23\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 471.23\n",
      "episode_reward_min: 137.0\n",
      "episodes_this_iter: 9\n",
      "episodes_total: 708\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 9.1552734375e-06\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.3512509003281593\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005659171175660742\n",
      "        policy_loss: -0.0013712859121702051\n",
      "        total_loss: 2.083930950551713\n",
      "        vf_explained_var: -0.4255241605543321\n",
      "        vf_loss: 2.085302214952401\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 45105.5\n",
      "  num_agent_steps_sampled: 196000\n",
      "  num_agent_steps_trained: 196000\n",
      "  num_env_steps_sampled: 196000\n",
      "  num_env_steps_trained: 196000\n",
      "iterations_since_restore: 49\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 196000\n",
      "num_agent_steps_trained: 196000\n",
      "num_env_steps_sampled: 196000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 196000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 21.292857142857144\n",
      "  ram_util_percent: 23.600000000000005\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08455599631780403\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04471080502211546\n",
      "  mean_inference_ms: 0.7531090550443486\n",
      "  mean_raw_obs_processing_ms: 0.2051978504417497\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0037021636962890625\n",
      "    StateBufferConnector_ms: 0.0030972957611083984\n",
      "    ViewRequirementAgentConnector_ms: 0.08389115333557129\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 471.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 471.23\n",
      "  episode_reward_min: 137.0\n",
      "  episodes_this_iter: 9\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 158,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 400, 188, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 418, 426, 445, 384,\n",
      "      500, 401, 427, 324, 368, 500, 284, 444, 500, 500, 429, 430, 391, 318, 355, 415,\n",
      "      500, 137, 500, 500, 500, 500, 500, 481]\n",
      "    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 158.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 400.0, 188.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 418.0, 426.0, 445.0,\n",
      "      384.0, 500.0, 401.0, 427.0, 324.0, 368.0, 500.0, 284.0, 444.0, 500.0, 500.0,\n",
      "      429.0, 430.0, 391.0, 318.0, 355.0, 415.0, 500.0, 137.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 481.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08455599631780403\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.04471080502211546\n",
      "    mean_inference_ms: 0.7531090550443486\n",
      "    mean_raw_obs_processing_ms: 0.2051978504417497\n",
      "time_since_restore: 496.5612258911133\n",
      "time_this_iter_s: 9.962233781814575\n",
      "time_total_s: 496.5612258911133\n",
      "timers:\n",
      "  learn_throughput: 697.865\n",
      "  learn_time_ms: 5731.764\n",
      "  load_throughput: 2914532.694\n",
      "  load_time_ms: 1.372\n",
      "  sample_time_ms: 4356.619\n",
      "  synch_weights_time_ms: 1.523\n",
      "  training_iteration_time_ms: 10091.896\n",
      "timestamp: 1683065667\n",
      "timesteps_total: 196000\n",
      "training_iteration: 49\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 200000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0036754608154296875\n",
      "  StateBufferConnector_ms: 0.0030930042266845703\n",
      "  ViewRequirementAgentConnector_ms: 0.08377909660339355\n",
      "counters:\n",
      "  num_agent_steps_sampled: 200000\n",
      "  num_agent_steps_trained: 200000\n",
      "  num_env_steps_sampled: 200000\n",
      "  num_env_steps_trained: 200000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-03_00-14-37\n",
      "done: false\n",
      "episode_len_mean: 468.53\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 468.53\n",
      "episode_reward_min: 137.0\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 716\n",
      "hostname: seymour\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 9.1552734375e-06\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.33653270743226493\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0035726212585089514\n",
      "        policy_loss: 0.0007548588898874098\n",
      "        total_loss: 1.5320219786146716\n",
      "        vf_explained_var: -0.580780622459227\n",
      "        vf_loss: 1.5312670880380357\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 46035.5\n",
      "  num_agent_steps_sampled: 200000\n",
      "  num_agent_steps_trained: 200000\n",
      "  num_env_steps_sampled: 200000\n",
      "  num_env_steps_trained: 200000\n",
      "iterations_since_restore: 50\n",
      "node_ip: 192.168.0.222\n",
      "num_agent_steps_sampled: 200000\n",
      "num_agent_steps_trained: 200000\n",
      "num_env_steps_sampled: 200000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_trained: 200000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 18.706666666666663\n",
      "  ram_util_percent: 23.60000000000001\n",
      "pid: 228860\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0845495829301409\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.044698040500147834\n",
      "  mean_inference_ms: 0.7529924302163309\n",
      "  mean_raw_obs_processing_ms: 0.20514932486261478\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0036754608154296875\n",
      "    StateBufferConnector_ms: 0.0030930042266845703\n",
      "    ViewRequirementAgentConnector_ms: 0.08377909660339355\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 468.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 468.53\n",
      "  episode_reward_min: 137.0\n",
      "  episodes_this_iter: 8\n",
      "  hist_stats:\n",
      "    episode_lengths: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 158, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 400, 188, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
      "      500, 500, 500, 500, 418, 426, 445, 384, 500, 401, 427, 324, 368, 500, 284, 444,\n",
      "      500, 500, 429, 430, 391, 318, 355, 415, 500, 137, 500, 500, 500, 500, 500, 481,\n",
      "      379, 500, 500, 351, 500, 500, 500, 500]\n",
      "    episode_reward: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 158.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 400.0, 188.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0,\n",
      "      418.0, 426.0, 445.0, 384.0, 500.0, 401.0, 427.0, 324.0, 368.0, 500.0, 284.0,\n",
      "      444.0, 500.0, 500.0, 429.0, 430.0, 391.0, 318.0, 355.0, 415.0, 500.0, 137.0,\n",
      "      500.0, 500.0, 500.0, 500.0, 500.0, 481.0, 379.0, 500.0, 500.0, 351.0, 500.0,\n",
      "      500.0, 500.0, 500.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0845495829301409\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.044698040500147834\n",
      "    mean_inference_ms: 0.7529924302163309\n",
      "    mean_raw_obs_processing_ms: 0.20514932486261478\n",
      "time_since_restore: 506.54927253723145\n",
      "time_this_iter_s: 9.988046646118164\n",
      "time_total_s: 506.54927253723145\n",
      "timers:\n",
      "  learn_throughput: 699.404\n",
      "  learn_time_ms: 5719.157\n",
      "  load_throughput: 2941255.588\n",
      "  load_time_ms: 1.36\n",
      "  sample_time_ms: 4348.229\n",
      "  synch_weights_time_ms: 1.493\n",
      "  training_iteration_time_ms: 10070.855\n",
      "timestamp: 1683065677\n",
      "timesteps_total: 200000\n",
      "training_iteration: 50\n",
      "trial_id: default\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "dqn = (\n",
    "    PPOConfig()\n",
    "    .rollouts(num_rollout_workers=1)\n",
    "    .resources(num_gpus=1)\n",
    "    .environment(env='CartPole-v1')\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(50):\n",
    "    result = dqn.train()\n",
    "    print(pretty_print(result))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='human')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.0\n"
     ]
    }
   ],
   "source": [
    "state, info = env.reset()\n",
    "\n",
    "done = False\n",
    "return_ = 0\n",
    "while not done:\n",
    "    action = dqn.compute_single_action(state)\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    return_ += reward\n",
    "    env.render()\n",
    "print(return_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[classic_control] in /home/fassty/anaconda3/envs/diploma_thesis/lib/python3.10/site-packages (0.26.3)\r\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/fassty/anaconda3/envs/diploma_thesis/lib/python3.10/site-packages (from gymnasium[classic_control]) (1.24.3)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/fassty/anaconda3/envs/diploma_thesis/lib/python3.10/site-packages (from gymnasium[classic_control]) (2.2.1)\r\n",
      "Requirement already satisfied: gymnasium-notices>=0.0.1 in /home/fassty/anaconda3/envs/diploma_thesis/lib/python3.10/site-packages (from gymnasium[classic_control]) (0.0.1)\r\n",
      "Collecting pygame==2.1.0\r\n",
      "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\r\n",
      "\u001B[2K     \u001B[90m\u001B[0m \u001B[32m18.3/18.3 MB\u001B[0m \u001B[31m22.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: pygame\r\n",
      "Successfully installed pygame-2.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install \"gymnasium[classic_control]\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
